{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPJBr6J78AAB"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdbhEgt48Eud"
      },
      "source": [
        "*   Pada file ops/make.sh ganti path yang sesuai\n",
        "*   Bila menggunakan jupyter/colab setelah melakukan instalasi package harap restart kernel\n",
        "*   Untuk pergantian path tedapat pada tab Parameters\n",
        "*   Perubahan dataset dan model terdapat pada setiap tab experiments (K-Fold dan Cross dataset)\n",
        "*   NOTE: bila menggunakan deterministic, variable lr_latent diganti menjadi None dan bila menggunakan probabilistic, vaiable lr_latent diganti menjadi 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2fs4Ps8GY0Y"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tOZkkgc_9wg",
        "outputId": "fdbe25f1-0f31-4e03-9a49-36c307560a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting absl-py==0.13.0\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting addict==2.4.0\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting cachetools==4.2.2\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 87.8 MB/s \n",
            "\u001b[?25hCollecting google-auth==1.32.1\n",
            "  Downloading google_auth-1.32.1-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib==0.4.4\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Collecting grpcio==1.38.1\n",
            "  Downloading grpcio-1.38.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 10)) (2.10)\n",
            "Collecting importlib-metadata==4.6.1\n",
            "  Downloading importlib_metadata-4.6.1-py3-none-any.whl (17 kB)\n",
            "Collecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 73.4 MB/s \n",
            "\u001b[?25hCollecting Markdown==3.3.4\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting mmcv==1.3.9\n",
            "  Downloading mmcv-1.3.9.tar.gz (313 kB)\n",
            "\u001b[K     |████████████████████████████████| 313 kB 74.9 MB/s \n",
            "\u001b[?25hCollecting monai==0.5.3\n",
            "  Downloading monai-0.5.3-202106011449-py3-none-any.whl (497 kB)\n",
            "\u001b[K     |████████████████████████████████| 497 kB 53.6 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 64.3 MB/s \n",
            "\u001b[?25hCollecting oauthlib==3.1.1\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting opencv-python==4.5.3.56\n",
            "  Downloading opencv_python-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.9 MB 77.9 MB/s \n",
            "\u001b[?25hCollecting Pillow==8.3.1\n",
            "  Downloading Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 62.2 MB/s \n",
            "\u001b[?25hCollecting prettytable==2.1.0\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: protobuf==3.17.3 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 21)) (3.17.3)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 22)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 23)) (0.2.8)\n",
            "Collecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib==1.3.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting rsa==4.7.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 11.7 MB/s \n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 31)) (0.0)\n",
            "Collecting tensorboard==2.5.0\n",
            "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 33)) (0.6.1)\n",
            "Collecting tensorboard-plugin-wit==1.8.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 59.6 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl==2.2.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting typing-extensions==3.10.0.0\n",
            "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 73.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/Thesis/requirements.txt (line 38)) (0.2.5)\n",
            "Collecting Werkzeug==2.0.1\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting yapf==0.31.0\n",
            "  Downloading yapf-0.31.0-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 83.2 MB/s \n",
            "\u001b[?25hCollecting zipp==3.5.0\n",
            "  Downloading zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.32.1->-r drive/MyDrive/Thesis/requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: torch>=1.5 in /usr/local/lib/python3.7/dist-packages (from monai==0.5.3->-r drive/MyDrive/Thesis/requirements.txt (line 15)) (1.11.0+cu113)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.5.0->-r drive/MyDrive/Thesis/requirements.txt (line 32)) (0.37.1)\n",
            "Building wheels for collected packages: future, mmcv\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=fa0c13135072e68995b055bb2649211e7fd520dfe182e55097fbd12f36899ff4\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv: filename=mmcv-1.3.9-py2.py3-none-any.whl size=451832 sha256=ff8b441be61cd04dd0d94579cb81a051d8a6887103dda1b16610d861e06a3e9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/48/bf/655e136aea5534d7a9a85fe247fee7957178fc19cf79dda602\n",
            "Successfully built future mmcv\n",
            "Installing collected packages: urllib3, chardet, certifi, zipp, typing-extensions, six, rsa, requests, oauthlib, numpy, cachetools, threadpoolctl, scipy, requests-oauthlib, joblib, importlib-metadata, google-auth, yapf, Werkzeug, tensorboard-plugin-wit, scikit-learn, PyYAML, Pillow, Markdown, grpcio, google-auth-oauthlib, addict, absl-py, tensorboard, prettytable, opencv-python, monai, mmcv, future\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.5.18.1\n",
            "    Uninstalling certifi-2022.5.18.1:\n",
            "      Successfully uninstalled certifi-2022.5.18.1\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.0\n",
            "    Uninstalling zipp-3.8.0:\n",
            "      Successfully uninstalled zipp-3.8.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.0\n",
            "    Uninstalling oauthlib-3.2.0:\n",
            "      Successfully uninstalled oauthlib-3.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.4\n",
            "    Uninstalling importlib-metadata-4.11.4:\n",
            "      Successfully uninstalled importlib-metadata-4.11.4\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.3.0\n",
            "    Uninstalling prettytable-3.3.0:\n",
            "      Successfully uninstalled prettytable-3.3.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "torchvision 0.12.0+cu113 requires pillow!=8.3.*,>=5.3.0, but you have pillow 8.3.1 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Markdown-3.3.4 Pillow-8.3.1 PyYAML-5.4.1 Werkzeug-2.0.1 absl-py-0.13.0 addict-2.4.0 cachetools-4.2.2 certifi-2021.5.30 chardet-4.0.0 future-0.18.2 google-auth-1.32.1 google-auth-oauthlib-0.4.4 grpcio-1.38.1 importlib-metadata-4.6.1 joblib-1.0.1 mmcv-1.3.9 monai-0.5.3 numpy-1.19.5 oauthlib-3.1.1 opencv-python-4.5.3.56 prettytable-2.1.0 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-learn-0.24.2 scipy-1.5.4 six-1.16.0 tensorboard-2.5.0 tensorboard-plugin-wit-1.8.0 threadpoolctl-2.2.0 typing-extensions-3.10.0.0 urllib3-1.26.6 yapf-0.31.0 zipp-3.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "numpy",
                  "six",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r drive/MyDrive/Thesis/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzeZ-2PTRyii",
        "outputId": "9a302d2b-3814-4b7b-f2a9-7c359aff76aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |███████████████████████         | 834.1 MB 82.7 MB/s eta 0:00:04tcmalloc: large alloc 1147494400 bytes == 0x39346000 @  0x7f6ec2b24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.2 MB/s eta 0:01:27tcmalloc: large alloc 1434370048 bytes == 0x7d99c000 @  0x7f6ec2b24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1156.7 MB 98.4 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0xd3188000 @  0x7f6ec2b24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.10.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (8.3.1)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1+cu110 torchaudio-0.7.2 torchvision-0.8.2+cu110\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLPuLHzUJ8hi",
        "outputId": "789a291f-1c40-41d2-a448-2623ac1c9375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running build\n",
            "running build_ext\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:352: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/content\n",
            "creating build/temp.linux-x86_64-3.7/content/drive\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cuda\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/drive/MyDrive/Thesis/ops/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/drive/MyDrive/Thesis/ops/src/vision.cpp -o build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:149:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "In file included from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/vision.cpp:11:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:29:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     if (value.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda())\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/vision.cpp:11:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:51:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     if (value.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda())\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/drive/MyDrive/Thesis/ops/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/drive/MyDrive/Thesis/ops/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(261): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(64): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(762): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(872): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(331): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(436): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(544): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_im2col_cuda.cuh(649): warning: variable \"q_col\" was declared but never referenced\n",
            "          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" \n",
            "/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\n",
            "\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:34:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA \u001b[01;35m\u001b[Kt\u001b[m\u001b[Kensor\");\n",
            "                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:35:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must b\u001b[01;35m\u001b[Ke\u001b[m\u001b[K a CUDA tensor\");\n",
            "                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:36:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index mus\u001b[01;35m\u001b[Kt\u001b[m\u001b[K be a CUDA tensor\");\n",
            "                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:37:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be \u001b[01;35m\u001b[Ka\u001b[m\u001b[K CUDA tensor\");\n",
            "                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:38:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.t\u001b[01;35m\u001b[Ky\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:269:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:355:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:398:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:431:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:514:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:672:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:835:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:921:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:964:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:996:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:1078:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:64:1235:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:100:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA \u001b[01;35m\u001b[Kt\u001b[m\u001b[Kensor\");\n",
            "                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:101:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must b\u001b[01;35m\u001b[Ke\u001b[m\u001b[K a CUDA tensor\");\n",
            "                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:102:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index mus\u001b[01;35m\u001b[Kt\u001b[m\u001b[K be a CUDA tensor\");\n",
            "                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:103:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be \u001b[01;35m\u001b[Ka\u001b[m\u001b[K CUDA tensor\");\n",
            "                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:104:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:105:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.t\u001b[01;35m\u001b[Ky\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:277:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:303:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:389:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:432:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:465:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:548:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:709:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:793:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:881:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1104:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1129:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1215:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1258:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1290:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1372:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1532:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1615:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.cu:134:1702:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/vision.o build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-3.7/content/drive/MyDrive/Thesis/ops/src/cuda/ms_deform_attn_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/MultiScaleDeformableAttention.cpython-37m-x86_64-linux-gnu.so\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.7/MultiScaleDeformableAttention.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-37m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-37: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: MultiScaleDeformableAttention in /usr/local/lib/python3.7/dist-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg (1.0)\n"
          ]
        }
      ],
      "source": [
        "!bash drive/MyDrive/Thesis/ops/make.sh\n",
        "!pip install MultiScaleDeformableAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2dned6EHXdd"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swiscecfuc5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from glob import glob\n",
        "import albumentations as A\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1LZOmP0_1k6"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from mmcv.cnn import normal_init\n",
        "from typing import Optional\n",
        "from torch import nn, Tensor\n",
        "from mmcv.cnn import ConvModule\n",
        "from torch.nn.init import normal_\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.init import xavier_uniform_, constant_\n",
        "from mmcv.cnn import constant_init, kaiming_init, build_conv_layer, build_norm_layer\n",
        "from mmcv.runner import load_checkpoint\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "import logging\n",
        "\n",
        "from mmcv.utils import get_logger\n",
        "import MultiScaleDeformableAttention as MSDA\n",
        "from torch.autograd import Function\n",
        "from torch.autograd.function import once_differentiable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10MH0TkmrTXG"
      },
      "source": [
        "# 1. Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNkCREt8ra6-"
      },
      "outputs": [],
      "source": [
        "def read_data(file_path, start=0, end=0, step=1):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    end = data.shape[-1] if end == 0 else end\n",
        "    reverse_channel = [data[:,:,i] for i in range(start,end,step)]\n",
        "    return np.array(reverse_channel, dtype='float32')\n",
        "\n",
        "def adjust_data(img, cf=1, mask=False):\n",
        "    if mask is False:\n",
        "        # Normalized data \n",
        "        img = (img - np.mean(img))/(np.std(img) + K.epsilon())\n",
        "\n",
        "    img = np.expand_dims(img, -1)\n",
        "    # Central crop if needed\n",
        "    if cf<1: img = tf.image.central_crop(img, cf)\n",
        "    return img\n",
        "\n",
        "def check_data(img, seg):\n",
        "    # Select data that have a value (image that contain only background)\n",
        "    idx = [k for k,i in enumerate(img) if len(np.unique(i)) > 1] \n",
        "    return img[idx], seg[idx]\n",
        "\n",
        "def data_augmentation(images, masks):\n",
        "    transform = A.Compose([A.HorizontalFlip(), A.Rotate(p=0.8)])\n",
        "    for k,(i,j) in enumerate(zip(images,masks)):\n",
        "        transformed = transform(image=i, mask=j)\n",
        "        images[k] = transformed['image']\n",
        "        masks[k] = transformed['mask']\n",
        "\n",
        "def find_best(path_model, type_='min', model_type='det'):\n",
        "    # Find best weights in filename\n",
        "    if model_type == 'det':\n",
        "        path = glob(path_model+'/*.hdf5') \n",
        "    else:\n",
        "        path = glob(path_model+'/*') \n",
        "        \n",
        "    split = [i.split('-')[-1] for i in path]\n",
        "    split.sort(key=natural_keys)\n",
        "\n",
        "    if type_ == 'min':\n",
        "        file_ = split[0]\n",
        "    else:\n",
        "        file_ = split[-1]\n",
        "    return [i for i in path if file_ in i] [0]\n",
        "\n",
        "def show_img(img, y_true, y_pred, idx=0):\n",
        "    # Visualize image, ground truth, and prediction by slice\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,3,1); plt.title('FLAIR image')\n",
        "    plt.imshow(img[idx,:,:,0], cmap='gray'); plt.axis('off')\n",
        "    plt.subplot(1,3,2); plt.title('Ground Truth')\n",
        "    plt.imshow(y_true[idx,:,:,0], cmap='gray'); plt.axis('off')\n",
        "    plt.subplot(1,3,3); plt.title('Prediction')\n",
        "    plt.imshow(y_pred[idx,:,:,0], cmap='gray'); plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def save_wmh(y_pred, file_in, file_out, name='ADNI'):\n",
        "    # Save WMHs predictions \n",
        "    y_true = nib.load(file_in)\n",
        "    wmh = np.zeros(y_true.shape)\n",
        "\n",
        "    if name == 'Singapore':\n",
        "        if wmh.shape[1] != y_pred.shape[2]:\n",
        "            wmh_temp = tf.image.rot90(y_pred, k=3).numpy()\n",
        "            for i in range(wmh.shape[-1]):\n",
        "                wmh[:,12:220,i] = wmh_temp[i,:,:,0]\n",
        "        else:\n",
        "            for i in range(wmh.shape[-1]):\n",
        "                wmh[12:220,:,i] = y_pred[i,:,:,0]\n",
        "\n",
        "    elif name == 'GE3T':\n",
        "        for i in range(wmh.shape[-1]):\n",
        "            wmh[:128,:,i] = y_pred[i,:,:,0]\n",
        "    else:\n",
        "        for i in range(wmh.shape[-1]):\n",
        "            wmh[:,:,i] = y_pred[i,:,:,0]\n",
        "\n",
        "    wmh = nib.Nifti1Image(wmh, y_true.affine, y_true.header)\n",
        "    nib.save(wmh, file_out)\n",
        "\n",
        "def save_wmh_challenge(y_pred, file_in, file_out, name):\n",
        "    # Save WMHs prediction only for Challenge dataset as full\n",
        "    y_true = nib.load(file_in)\n",
        "    wmh = np.zeros(y_true.shape)\n",
        "\n",
        "    if name == 'Singapore': start=4; end=236\n",
        "    elif name == 'GE3T': start=54; end=186\n",
        "    elif name == 'Utrecht': start=8; end=248\n",
        "    else: start=0; end=240\n",
        "\n",
        "    if name == 'Singapore':\n",
        "        if wmh.shape[1] != y_pred.shape[2]:\n",
        "            wmh_temp = tf.image.rot90(y_pred, k=3).numpy()\n",
        "            for i in range(wmh.shape[-1]):\n",
        "                wmh[:,:,i] = wmh_temp[i,:,start:end,0]\n",
        "        else:\n",
        "            for i in range(wmh.shape[-1]):\n",
        "                wmh[:,:,i] = y_pred[i,start:end,:,0]\n",
        "\n",
        "    elif name == 'Utrecht':\n",
        "        for i in range(wmh.shape[-1]):\n",
        "            wmh[:,:,i] = y_pred[i,:,start:end,0]\n",
        "    else:\n",
        "        for i in range(wmh.shape[-1]):\n",
        "            wmh[:,:,i] = y_pred[i,start:end,:,0]\n",
        "\n",
        "    wmh = nib.Nifti1Image(wmh, y_true.affine, y_true.header)\n",
        "    nib.save(wmh, file_out)\n",
        "\n",
        "def raw2nii(type_, train=True, bias_cor=True, path_dataset=None):\n",
        "    # Convert .raw into .nii.gz\n",
        "    train = 'training' if train else 'testing'\n",
        "    path_files = path_dataset+'NITRC/'+train+'/{}/{}'\n",
        "    path_raw = glob(path_files.format('*', '*'+type_+'.nhdr'))\n",
        "    for i in path_raw:\n",
        "        file_name = i.split('/')[-1].split('.')[0] + '.nii.gz'\n",
        "        if bias_cor:\n",
        "            bias_field_correction(i, path_files.format('full',file_name))\n",
        "        else:\n",
        "            img = sitk.ReadImage(i)\n",
        "            sitk.WriteImage(img, path_files.format('full',file_name))\n",
        "\n",
        "def bias_field_correction(path, out):\n",
        "    # BFC using ANTs library\n",
        "    img = nib.load(path)\n",
        "    bfc = ants.image_read(path)\n",
        "    bfc = ants.n3_bias_field_correction(bfc)\n",
        "    bfc = nib.Nifti1Image(bfc.numpy(), img.affine, img.header)\n",
        "    nib.save(bfc, out)\n",
        "\n",
        "# https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "\n",
        "# https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
        "def natural_keys(text):\n",
        "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
        "\n",
        "# https://github.com/MrGiovanni/UNetPlusPlus/blob/master/keras/helper_functions.py#L37-L42\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "# https://github.com/umbertogriffo/focal-loss-keras/blob/master/src/loss_function/losses.py#L11-L53\n",
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
        "        epsilon = K.epsilon()\n",
        "        # Add the epsilon to prediction value\n",
        "        # y_pred = y_pred + epsilon\n",
        "        # Clip the prediciton value\n",
        "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
        "        # Calculate p_t\n",
        "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        # Calculate alpha_t\n",
        "        alpha_factor = K.ones_like(y_true) * alpha\n",
        "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
        "        # Calculate cross entropy\n",
        "        cross_entropy = -K.log(p_t)\n",
        "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
        "        # Calculate focal loss\n",
        "        loss = weight * cross_entropy\n",
        "        # Sum the losses in mini_batch\n",
        "        loss = K.mean(K.sum(loss, axis=1))\n",
        "        return loss\n",
        "    return binary_focal_loss_fixed\n",
        "\n",
        "# modified code from https://stackoverflow.com/questions/53303724/how-to-average-only-non-zero-entries-in-tensor\n",
        "# and https://stackoverflow.com/questions/42606207/keras-custom-decision-threshold-for-precision-and-recall\n",
        "def volume_loss(threshold=0.25):\n",
        "    def calculate(y_true, y_pred):\n",
        "        nonzero_true = K.any(K.not_equal(y_true, 0), axis=-1)\n",
        "        y_true_volume = K.sum(K.cast(nonzero_true, 'float32'), axis=-1, keepdims=True)\n",
        "        y_true_volume = K.sum(y_true_volume/1000)\n",
        "\n",
        "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())\n",
        "        nonzero_pred = K.any(K.not_equal(y_pred, 0), axis=-1)\n",
        "        y_pred_volume = K.sum(K.cast(nonzero_pred, 'float32'), axis=-1, keepdims=True)\n",
        "        y_pred_volume = K.sum(y_pred_volume/1000)\n",
        "        \n",
        "        return K.abs(y_true_volume-y_pred_volume)\n",
        "    return calculate\n",
        "\n",
        "def total_loss(type_loss='FL', gamma=2., alpha=.25, threshold=0.25):\n",
        "    def calculate(y_true, y_pred):\n",
        "        if type_loss == 'FTL':\n",
        "            loss = focal_tversky(gamma=gamma, alpha=alpha) (y_true, y_pred)\n",
        "        else:\n",
        "            loss = binary_focal_loss(gamma=gamma, alpha=alpha) (y_true, y_pred)\n",
        "        vl = volume_loss(threshold=threshold) (y_true, y_pred)\n",
        "        \n",
        "        return loss+vl\n",
        "    return calculate\n",
        "\n",
        "# https://github.com/baumgach/PHiSeg-code/blob/c43f3b32e1f434aecba936ff994b6f743ba7a5f8/utils.py#L326-L370\n",
        "def ambiguity_map(y_gen, seg=None):\n",
        "    def pixel_wise_xent(m_samp, m_gt, eps=1e-8):\n",
        "        log_samples = np.log(m_samp + eps)\n",
        "        return -1.0*np.sum(m_gt*log_samples, axis=-1)\n",
        "\n",
        "    y_pred = np.average(y_gen, axis=0)\n",
        "    E_arr = np.zeros(y_gen.shape)\n",
        "    for i in range(y_gen.shape[0]):\n",
        "        for j in range(y_gen.shape[1]):\n",
        "            if seg is None:\n",
        "                E_arr[i,j,...] = np.expand_dims(pixel_wise_xent(y_gen[i,j,...], y_pred[j,...]), axis=-1)\n",
        "            else:\n",
        "                E_arr[i,j,...] = np.expand_dims(pixel_wise_xent(y_gen[i,j,...], seg[j,...]), axis=-1)\n",
        "\n",
        "    return np.average(E_arr, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-e4SiGnAgLv"
      },
      "source": [
        "# 2. Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM4oLm2ZOo61"
      },
      "outputs": [],
      "source": [
        "def get_dataset(name, end=0):\n",
        "    if name == 'ADNI' and end >= 1:\n",
        "        start = 5; end = 30; step=1\n",
        "    elif name == 'NITRC'and end >= 1:\n",
        "        start = 140; end = 350; step=5\n",
        "    elif name == 'Singapore' and end >= 1:\n",
        "        start = 0; end = 45; step=1\n",
        "    elif name == 'GE3T' and end >= 1:\n",
        "        start = 25; end = 70; step=1\n",
        "    elif name == 'Utrecht' and end >= 1:\n",
        "        start = 0; end = 45; step=1\n",
        "    else:\n",
        "        start = 0; end = 0; step=1\n",
        "\n",
        "    if name == 'Challenge':\n",
        "        fold_img, fold_seg = read_challenge(start, end, step=1, cf=1)\n",
        "    else:\n",
        "        fold_img, fold_seg = read_dataset(name, start, end, step=step, cf=1)\n",
        "    \n",
        "    return fold_img, fold_seg\n",
        "\n",
        "def read_dataset(name, start, end, step=1, cf=1):\n",
        "    if name == 'ADNI':\n",
        "        flair_path = glob(path_dataset+'ADNI/*/*/*brain.nii.gz')\n",
        "        lesion_path = glob(path_dataset+'ADNI/*/*/*wmh.nii.gz')\n",
        "    elif name == 'NITRC':\n",
        "        flair_path = glob(path_dataset+'NITRC/training/*/*brain.nii.gz')\n",
        "        lesion_path = glob(path_dataset+'NITRC/training/*/*lesion.nii.gz')\n",
        "    else:\n",
        "        flair_path = glob(path_dataset+'Challenge/{}/*/*brain.nii.gz'.format(name))\n",
        "        lesion_path = glob(path_dataset+'Challenge/{}/*/*wmh.nii.gz'.format(name))\n",
        "\n",
        "    flair_path.sort(key=natural_keys) \n",
        "    lesion_path.sort(key=natural_keys)\n",
        "    \n",
        "    if name == 'ADNI':\n",
        "        flair_path = [[i for i in flair_path if i.split('/')[-3] == 'fold'+str(j)] for j in range(1,5)]\n",
        "        lesion_path = [[i for i in lesion_path if i.split('/')[-3] == 'fold'+str(j)] for j in range(1,5)]\n",
        "    else:\n",
        "        flair_path = [flair_path]\n",
        "        lesion_path = [lesion_path]\n",
        "\n",
        "    fold_img, fold_seg = [], []\n",
        "    for fp,lp in zip(flair_path,lesion_path):\n",
        "        if name == 'Singapore':\n",
        "            # Adjust first 2 data at Singapore\n",
        "            img_temp = [np.expand_dims(read_data(i, start=start, end=end, step=step), -1) for i in fp[:2]]\n",
        "            seg_temp = [np.expand_dims(read_data(i, start=start, end=end, step=step), -1) for i in lp[:2]]\n",
        "            img_temp = np.array([[np.squeeze(tf.image.rot90(j).numpy(), -1) for j in i] for i in img_temp])\n",
        "            seg_temp = np.array([[np.squeeze(tf.image.rot90(j).numpy(), -1) for j in i] for i in seg_temp])\n",
        "            del fp[:2]; del lp[:2]\n",
        "\n",
        "        # Read Data\n",
        "        img = np.array([read_data(i, start=start, end=end, step=step) for i in fp])\n",
        "        seg = np.array([read_data(i, start=start, end=end, step=step) for i in lp])\n",
        "\n",
        "        if name == 'Singapore':\n",
        "            # Concat with first 2 data and cropping\n",
        "            img = np.concatenate([img_temp, img])\n",
        "            seg = np.concatenate([seg_temp, seg])\n",
        "            img = np.array([i[:,12:220,:] for i in img])\n",
        "            seg = np.array([i[:,12:220,:] for i in seg])\n",
        "        elif name == 'GE3T':\n",
        "            # Cropping\n",
        "            img = np.array([i[:,:128,:] for i in img])\n",
        "            seg = np.array([i[:,:128,:] for i in seg])\n",
        "\n",
        "        # Normalized data\n",
        "        img = np.array([[adjust_data(j, cf=cf) for j in i] for i in img])\n",
        "        seg = np.array([[adjust_data(j, cf=cf, mask=True) for j in i] for i in seg])\n",
        "\n",
        "        if name not in ['ADNI', 'NITRC']:\n",
        "            seg[(seg == 2).all(axis=-1)] = 0\n",
        "\n",
        "        fold_img.append(img); fold_seg.append(seg)\n",
        "    return np.array(fold_img), np.array(fold_seg)\n",
        "\n",
        "def read_challenge(start, end, step=1, cf=1):\n",
        "    # Read Challenge dataset as full dataset\n",
        "    fold_img, fold_seg = [], []\n",
        "    for name in ['Singapore', 'GE3T', 'Utrecht']:\n",
        "        flair_path = glob(path_dataset+'Challenge/{}/*/*brain.nii.gz'.format(name))\n",
        "        lesion_path = glob(path_dataset+'Challenge/{}/*/*wmh.nii.gz'.format(name))\n",
        "        flair_path.sort(key=natural_keys) \n",
        "        lesion_path.sort(key=natural_keys)\n",
        "        flair_path = [flair_path]\n",
        "        lesion_path = [lesion_path]\n",
        "\n",
        "        for fp,lp in zip(flair_path, lesion_path):\n",
        "            if name == 'Singapore':\n",
        "                # Adjust first 2 data at Singapore\n",
        "                img_temp = [np.expand_dims(read_data(i, start=start, end=end, step=step), -1) for i in fp[:2]]\n",
        "                seg_temp = [np.expand_dims(read_data(i, start=start, end=end, step=step), -1) for i in lp[:2]]\n",
        "                img_temp = np.array([[np.squeeze(tf.image.rot90(j).numpy(), -1) for j in i] for i in img_temp])\n",
        "                seg_temp = np.array([[np.squeeze(tf.image.rot90(j).numpy(), -1) for j in i] for i in seg_temp])\n",
        "                del fp[:2]; del lp[:2]\n",
        "\n",
        "            # Read Data\n",
        "            img = np.array([read_data(i, start=start, end=end, step=step) for i in fp])\n",
        "            seg = np.array([read_data(i, start=start, end=end, step=step) for i in lp])\n",
        "            \n",
        "            if name == 'Singapore':\n",
        "                img = np.concatenate([img_temp, img])\n",
        "                seg = np.concatenate([seg_temp, seg])\n",
        "            fold_img.append(img); fold_seg.append(seg)\n",
        "\n",
        "    # Get the max sizes from all insitutions\n",
        "    sizes = [i.shape[-2:] for i in fold_img]\n",
        "    max_sizes = np.max(list(zip(*sizes)), -1).tolist()\n",
        "    diff_sizes = [(int((max_sizes[0]-i)/2), (int((max_sizes[1]-j)/2))) for i,j in sizes]\n",
        "    fold_img_padd, fold_seg_padd = [], []\n",
        "\n",
        "    for k,(i,j) in enumerate(zip(fold_img, fold_seg)):\n",
        "        # Zero padding\n",
        "        img = np.zeros(i.shape[:2] + tuple(max_sizes))\n",
        "        seg = np.zeros(i.shape[:2] + tuple(max_sizes))\n",
        "        x = sizes[k][0]; x_pad = diff_sizes[k][0]\n",
        "        y = sizes[k][1]; y_pad = diff_sizes[k][1]\n",
        "\n",
        "        img[:,:,x_pad:x+x_pad,y_pad:y+y_pad] = i\n",
        "        seg[:,:,x_pad:x+x_pad,y_pad:y+y_pad] = j\n",
        "\n",
        "        # Normalized data\n",
        "        img = np.array([[adjust_data(j, cf=cf) for j in i] for i in img])\n",
        "        seg = np.array([[adjust_data(j, cf=cf, mask=True) for j in i] for i in seg])\n",
        "        seg[(seg == 2).all(axis=-1)] = 0\n",
        "        fold_img_padd.append(img); fold_seg_padd.append(seg)\n",
        "\n",
        "    return fold_img_padd, fold_seg_padd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjpVcjyYvG4j"
      },
      "source": [
        "# 3. Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSZJB_KivCww"
      },
      "outputs": [],
      "source": [
        "# Paths config\n",
        "## For saving WMHs prediction\n",
        "result_path = '../content/drive/MyDrive/Thesis/Results/{}/{}/{}'\n",
        "## Pre-trained path\n",
        "path = '../content/drive/MyDrive/Publikasi/pre-trained/'\n",
        "## Dataset path\n",
        "path_dataset = '../content/drive/MyDrive/Datasets/'\n",
        "## Evaluation path\n",
        "eval_path = '../content/drive/MyDrive/Thesis/Evaluation_Excel/'\n",
        "\n",
        "dataset_type = ['ADNI', 'Singapore', 'GE3T', 'Utrecht', 'Challenge']\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_total_loss', factor=0.5, patience=2)\n",
        "generate_pred = 30\n",
        "threshold = 0.25\n",
        "batch_size = 4\n",
        "epoch = 100\n",
        "lr = 0.001\n",
        "\n",
        "lr_latent = None#0.001\n",
        "# Deterministic gamma = 1.0, Probabilistic gamma = 0.25\n",
        "gamma = 1.0\n",
        "alpha = 0.5\n",
        "beta = 1\n",
        "if lr_latent is None:\n",
        "    uniq = str(lr)+'_'+str(gamma)+'_'+str(alpha)\n",
        "else:\n",
        "    uniq = str(lr)+'_'+str(lr_latent)+'_'+str(gamma)+'_'+str(alpha)\n",
        "\n",
        "def pick_model(model_name, dataset_name=None):\n",
        "    n_blocks = 4\n",
        "    n_classes = 1 \n",
        "    input_shape = (None, None, 1)\n",
        "    decoder_filters = (256, 128, 64, 32)\n",
        "\n",
        "    # Deterministic\n",
        "    if model_name == 'U_Net':\n",
        "        md = Unet(use_backbone=False, input_shape=input_shape, attention=False,\n",
        "                n_upsample_blocks=n_blocks, decoder_filters=decoder_filters)\n",
        "    \n",
        "    elif model_name == 'MCTrans':\n",
        "        md = EncoderDecoder(n_points=6)\n",
        "\n",
        "    # Probabilistic\n",
        "    elif model_name == 'Prob_U_Net':\n",
        "        md = Prob_Unet(num_classes=n_classes, activation='sigmoid', latent_dim=6, concat_lvl=[0])\n",
        "    \n",
        "    elif model_name == 'Prob_MCTrans':\n",
        "        md = EncoderDecoder(prob=True, n_points=4, latent_dim=6)\n",
        "        \n",
        "    else:\n",
        "        md = None\n",
        "    return md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSse6KJA_uxV"
      },
      "source": [
        "# MCTrans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utSk7LxJDB4y"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5eaaU4UDBkC"
      },
      "outputs": [],
      "source": [
        "def resize(input,\n",
        "           size=None,\n",
        "           scale_factor=None,\n",
        "           mode='nearest',\n",
        "           align_corners=None,\n",
        "           warning=True):\n",
        "    if warning:\n",
        "        if size is not None and align_corners:\n",
        "            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n",
        "            output_h, output_w = tuple(int(x) for x in size)\n",
        "            if output_h > input_h or output_w > output_h:\n",
        "                if ((output_h > 1 and output_w > 1 and input_h > 1\n",
        "                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n",
        "                        and (output_w - 1) % (input_w - 1)):\n",
        "                    warnings.warn(\n",
        "                        f'When align_corners={align_corners}, '\n",
        "                        'the output would more aligned if '\n",
        "                        f'input size {(input_h, input_w)} is `x+1` and '\n",
        "                        f'out size {(output_h, output_w)} is `nx+1`')\n",
        "    if isinstance(size, torch.Size):\n",
        "        size = tuple(int(x) for x in size)\n",
        "    if isinstance(size, (numpy.ndarray, numpy.generic) ):\n",
        "        size = tuple(int(x) for x in size)\n",
        "    return F.interpolate(input, size, scale_factor, mode, align_corners)\n",
        "\n",
        "def get_root_logger(log_file=None, log_level=logging.INFO):\n",
        "    \"\"\"Get the root logger.\n",
        "    The logger will be initialized if it has not been initialized. By default a\n",
        "    StreamHandler will be added. If `log_file` is specified, a FileHandler will\n",
        "    also be added. The name of the root logger is the top-level package name,\n",
        "    e.g., \"mmseg\".\n",
        "    Args:\n",
        "        log_file (str | None): The log filename. If specified, a FileHandler\n",
        "            will be added to the root logger.\n",
        "        log_level (int): The root logger level. Note that only the process of\n",
        "            rank 0 is affected, while other processes will set the level to\n",
        "            \"Error\" and be silent most of the time.\n",
        "    Returns:\n",
        "        logging.Logger: The root logger.\n",
        "    \"\"\"\n",
        "\n",
        "    logger = get_logger(name='MCTrans', log_file=log_file, log_level=log_level)\n",
        "\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2KdAG1iCFyD"
      },
      "source": [
        "## Full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1hWwvvpABiS"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, num_classes=1, d_model=128, num_filters=[64,64,128,128,128], n_points=4, prob=False, latent_dim=6):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.prob = prob\n",
        "        self.encoder = ResNet(in_channels=1, depth=18, out_indices=(0, 1, 2, 3, 4))\n",
        "        self.decoder = UNetDecoder(in_channels=num_filters)\n",
        "\n",
        "        self.center = MCTrans(d_model=d_model, nhead=8, d_ffn=512, dropout=0.1, \n",
        "                              act=\"relu\", n_levels=3, n_points=n_points, n_sa_layers=6)\n",
        "        \n",
        "        self.head = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
        "        self.aux_head = MCTransAuxHead(d_model=d_model, d_ffn=512, act=\"relu\", \n",
        "                                       num_classes=num_classes, in_channles=num_filters)\n",
        "\n",
        "        if self.prob:\n",
        "            self.initializers = {'w':'he_normal', 'b':'normal'}\n",
        "            self.prior = AxisAlignedConvGaussian(input_channels=1, num_filters=num_filters[:-1], \n",
        "                                                no_convs_per_block=2, latent_dim=latent_dim,\n",
        "                                                 initializers=self.initializers)\n",
        "            self.posterior = AxisAlignedConvGaussian(input_channels=1, num_filters=num_filters[:-1],\n",
        "                                                    no_convs_per_block=2, latent_dim=latent_dim,\n",
        "                                                    initializers=self.initializers, posterior=True)\n",
        "            self.fcomb = Fcomb(num_filters[:-1], latent_dim, num_classes, num_classes, 2, self.initializers, use_tile=True)\n",
        "\n",
        "    def forward(self, img, seg):\n",
        "        x = self.extract_feat(img)\n",
        "        x_ = self.decoder(x)\n",
        "        x_ = self.head(x_)\n",
        "        logits = self.aux_head(x)\n",
        "        \n",
        "        if self.prob:\n",
        "            self.posterior_latent_space = self.posterior(img, seg)\n",
        "            self.prior_latent_space = self.prior(img)\n",
        "            x_ = self.fcomb(x_, self.posterior_latent_space.rsample())\n",
        "\n",
        "        return x_, logits\n",
        "\n",
        "    def sampling(self, img, seg):\n",
        "        x = self.extract_feat(img)\n",
        "        x_ = self.decoder(x)\n",
        "        x_ = self.head(x_)\n",
        "        logits = self.aux_head(x)\n",
        "        \n",
        "        if self.prob:\n",
        "            self.posterior_latent_space = self.posterior(img, seg)\n",
        "            self.prior_latent_space = self.prior(img)\n",
        "            x_ = self.fcomb(x_, self.prior_latent_space.sample())\n",
        "\n",
        "        return x_, logits\n",
        "\n",
        "    def extract_feat(self, img):\n",
        "        x = self.encoder(img)\n",
        "        x = self.center(x)\n",
        "        return x\n",
        "\n",
        "    def kl_divergence(self, analytic=False, calculate_posterior=True, z_posterior=None):\n",
        "        \"\"\"\n",
        "        Calculate the KL divergence between the posterior and prior KL(Q||P)\n",
        "        analytic: calculate KL analytically or via sampling from the posterior\n",
        "        calculate_posterior: if we use samapling to approximate KL we can sample here or supply a sample\n",
        "        \"\"\"\n",
        "        if analytic:\n",
        "            # Need to add this to torch source code, see: https://github.com/pytorch/pytorch/issues/13545\n",
        "            kl_div = kl._kl_independent_independent(self.posterior_latent_space, self.prior_latent_space)\n",
        "        else:\n",
        "            if calculate_posterior:\n",
        "                z_posterior = self.posterior_latent_space.rsample()\n",
        "            log_posterior_prob = self.posterior_latent_space.log_prob(z_posterior)\n",
        "            log_prior_prob = self.prior_latent_space.log_prob(z_posterior)\n",
        "            kl_div = log_posterior_prob - log_prior_prob\n",
        "        return kl_div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKr7vaEgHlM5"
      },
      "source": [
        "## Probabilistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1VxJtv0HkHb"
      },
      "outputs": [],
      "source": [
        "# https://github.com/stefanknegt/Probabilistic-Unet-Pytorch/blob/master/probabilistic_unet.py\n",
        "\n",
        "from torch.distributions import Normal, Independent\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional neural network, consisting of len(num_filters) times a block of no_convs_per_block convolutional layers,\n",
        "    after each block a pooling operation is performed. And after each convolutional layer a non-linear (ReLU) activation function is applied.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, num_filters, no_convs_per_block, initializers, padding=True, posterior=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.contracting_path = nn.ModuleList()\n",
        "        self.input_channels = input_channels\n",
        "        self.num_filters = num_filters\n",
        "\n",
        "        if posterior:\n",
        "            #To accomodate for the mask that is concatenated at the channel axis, we increase the input_channels.\n",
        "            self.input_channels += 1\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(self.num_filters)):\n",
        "            \"\"\"\n",
        "            Determine input_dim and output_dim of conv layers in this block. The first layer is input x output,\n",
        "            All the subsequent layers are output x output.\n",
        "            \"\"\"\n",
        "            input_dim = self.input_channels if i == 0 else output_dim\n",
        "            output_dim = num_filters[i]\n",
        "            \n",
        "            if i != 0:\n",
        "                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True))\n",
        "            \n",
        "            layers.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, padding=int(padding)))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            for _ in range(no_convs_per_block-1):\n",
        "                layers.append(nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=int(padding)))\n",
        "                layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        self.layers.apply(init_weights)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.layers(input)\n",
        "        return output\n",
        "\n",
        "class AxisAlignedConvGaussian(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional net that parametrizes a Gaussian distribution with axis aligned covariance matrix.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, num_filters, no_convs_per_block, latent_dim, initializers, posterior=False):\n",
        "        super(AxisAlignedConvGaussian, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.channel_axis = 1\n",
        "        self.num_filters = num_filters\n",
        "        self.no_convs_per_block = no_convs_per_block\n",
        "        self.latent_dim = latent_dim\n",
        "        self.posterior = posterior\n",
        "        if self.posterior:\n",
        "            self.name = 'Posterior'\n",
        "        else:\n",
        "            self.name = 'Prior'\n",
        "        self.encoder = Encoder(self.input_channels, self.num_filters, self.no_convs_per_block, initializers, posterior=self.posterior)\n",
        "        self.conv_layer = nn.Conv2d(num_filters[-1], 2 * self.latent_dim, (1,1), stride=1)\n",
        "        self.show_img = 0\n",
        "        self.show_seg = 0\n",
        "        self.show_concat = 0\n",
        "        self.show_enc = 0\n",
        "        self.sum_input = 0\n",
        "\n",
        "        nn.init.kaiming_normal_(self.conv_layer.weight, mode='fan_in', nonlinearity='relu')\n",
        "        nn.init.normal_(self.conv_layer.bias)\n",
        "\n",
        "    def forward(self, input, segm=None):\n",
        "\n",
        "        #If segmentation is not none, concatenate the mask to the channel axis of the input\n",
        "        if segm is not None:\n",
        "            self.show_img = input\n",
        "            self.show_seg = segm\n",
        "            input = torch.cat((input, segm), dim=1)\n",
        "            self.show_concat = input\n",
        "            self.sum_input = torch.sum(input)\n",
        "\n",
        "        encoding = self.encoder(input)\n",
        "        self.show_enc = encoding\n",
        "\n",
        "        #We only want the mean of the resulting hxw image\n",
        "        encoding = torch.mean(encoding, dim=2, keepdim=True)\n",
        "        encoding = torch.mean(encoding, dim=3, keepdim=True)\n",
        "\n",
        "        #Convert encoding to 2 x latent dim and split up for mu and log_sigma\n",
        "        mu_log_sigma = self.conv_layer(encoding)\n",
        "\n",
        "        #We squeeze the second dimension twice, since otherwise it won't work when batch size is equal to 1\n",
        "        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)\n",
        "        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)\n",
        "\n",
        "        mu = mu_log_sigma[:,:self.latent_dim]\n",
        "        log_sigma = mu_log_sigma[:,self.latent_dim:]\n",
        "\n",
        "        #This is a multivariate normal with diagonal covariance matrix sigma\n",
        "        #https://github.com/pytorch/pytorch/pull/11178\n",
        "        dist = Independent(Normal(loc=mu, scale=torch.exp(log_sigma)),1)\n",
        "        return dist\n",
        "\n",
        "class Fcomb(nn.Module):\n",
        "    \"\"\"\n",
        "    A function composed of no_convs_fcomb times a 1x1 convolution that combines the sample taken from the latent space,\n",
        "    and output of the UNet (the feature map) by concatenating them along their channel axis.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters, latent_dim, num_output_channels, num_classes, no_convs_fcomb, initializers, use_tile=True):\n",
        "        super(Fcomb, self).__init__()\n",
        "        self.num_channels = num_output_channels #output channels\n",
        "        self.num_classes = num_classes\n",
        "        self.channel_axis = 1\n",
        "        self.spatial_axes = [2,3]\n",
        "        self.num_filters = num_filters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.use_tile = use_tile\n",
        "        self.no_convs_fcomb = no_convs_fcomb \n",
        "        self.name = 'Fcomb'\n",
        "\n",
        "        if self.use_tile:\n",
        "            layers = []\n",
        "\n",
        "            #Decoder of N x a 1x1 convolution followed by a ReLU activation function except for the last layer\n",
        "            layers.append(nn.Conv2d(self.num_channels+self.latent_dim, self.num_filters[0], kernel_size=1))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            for _ in range(no_convs_fcomb-2):\n",
        "                layers.append(nn.Conv2d(self.num_filters[0], self.num_filters[0], kernel_size=1))\n",
        "                layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            self.layers = nn.Sequential(*layers)\n",
        "\n",
        "            self.last_layer = nn.Conv2d(self.num_filters[0], self.num_classes, kernel_size=1)\n",
        "\n",
        "            if initializers['w'] == 'orthogonal':\n",
        "                self.layers.apply(init_weights_orthogonal_normal)\n",
        "                self.last_layer.apply(init_weights_orthogonal_normal)\n",
        "            else:\n",
        "                self.layers.apply(init_weights)\n",
        "                self.last_layer.apply(init_weights)\n",
        "\n",
        "    def tile(self, a, dim, n_tile):\n",
        "        \"\"\"\n",
        "        This function is taken form PyTorch forum and mimics the behavior of tf.tile.\n",
        "        Source: https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/3\n",
        "        \"\"\"\n",
        "        init_dim = a.size(dim)\n",
        "        repeat_idx = [1] * a.dim()\n",
        "        repeat_idx[dim] = n_tile\n",
        "        a = a.repeat(*(repeat_idx))\n",
        "        order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(a.device)\n",
        "        return torch.index_select(a, dim, order_index)\n",
        "\n",
        "    def forward(self, feature_map, z):\n",
        "        \"\"\"\n",
        "        Z is batch_sizexlatent_dim and feature_map is batch_sizexno_channelsxHxW.\n",
        "        So broadcast Z to batch_sizexlatent_dimxHxW. Behavior is exactly the same as tf.tile (verified)\n",
        "        \"\"\"\n",
        "        if self.use_tile:\n",
        "            z = torch.unsqueeze(z,2)\n",
        "            z = self.tile(z, 2, feature_map.shape[self.spatial_axes[0]])\n",
        "            z = torch.unsqueeze(z,3)\n",
        "            z = self.tile(z, 3, feature_map.shape[self.spatial_axes[1]])\n",
        "\n",
        "            #Concatenate the feature map (output of the UNet) and the sample taken from the latent space\n",
        "            feature_map = torch.cat((feature_map, z), dim=self.channel_axis)\n",
        "            output = self.layers(feature_map)\n",
        "            return self.last_layer(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBqfM3_nMudC"
      },
      "outputs": [],
      "source": [
        "# https://github.com/stefanknegt/Probabilistic-Unet-Pytorch/blob/80152817d90d396a6285cfa24a42c6f255b02ba4/utils.py\n",
        "\n",
        "def truncated_normal_(tensor, mean=0, std=1):\n",
        "    size = tensor.shape\n",
        "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
        "    valid = (tmp < 2) & (tmp > -2)\n",
        "    ind = valid.max(-1, keepdim=True)[1]\n",
        "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "    tensor.data.mul_(std).add_(mean)\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        #nn.init.normal_(m.weight, std=0.001)\n",
        "        #nn.init.normal_(m.bias, std=0.001)\n",
        "        truncated_normal_(m.bias, mean=0, std=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wGprImIEOCK"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70QmLsPeENl8"
      },
      "outputs": [],
      "source": [
        "filter_nums = {18: (64, 64, 128, 256, 512),\n",
        "               34: (64, 64, 128, 256, 512)}\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 inplanes,\n",
        "                 planes,\n",
        "                 stride=1,\n",
        "                 dilation=1,\n",
        "                 downsample=None,\n",
        "                 style='pytorch',\n",
        "                 with_cp=False,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=dict(type='BN')):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n",
        "        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n",
        "\n",
        "        self.conv1 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            inplanes,\n",
        "            planes,\n",
        "            3,\n",
        "            stride=stride,\n",
        "            padding=dilation,\n",
        "            dilation=dilation,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm1_name, norm1)\n",
        "        self.conv2 = build_conv_layer(\n",
        "            conv_cfg, planes, planes, 3, padding=1, bias=False)\n",
        "        self.add_module(self.norm2_name, norm2)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        assert not with_cp\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        return getattr(self, self.norm1_name)\n",
        "\n",
        "    @property\n",
        "    def norm2(self):\n",
        "        return getattr(self, self.norm2_name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.norm1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self,\n",
        "                 inplanes,\n",
        "                 planes,\n",
        "                 stride=1,\n",
        "                 dilation=1,\n",
        "                 downsample=None,\n",
        "                 style='pytorch',\n",
        "                 with_cp=False,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=dict(type='BN')):\n",
        "        \"\"\"Bottleneck block for ResNet.\n",
        "        If style is \"pytorch\", the stride-two layer is the 3x3 conv layer,\n",
        "        if it is \"caffe\", the stride-two layer is the first 1x1 conv layer.\n",
        "        \"\"\"\n",
        "        super(Bottleneck, self).__init__()\n",
        "        assert style in ['pytorch', 'caffe']\n",
        "\n",
        "        self.inplanes = inplanes\n",
        "        self.planes = planes\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.style = style\n",
        "        self.with_cp = with_cp\n",
        "        self.conv_cfg = conv_cfg\n",
        "        self.norm_cfg = norm_cfg\n",
        "\n",
        "        if self.style == 'pytorch':\n",
        "            self.conv1_stride = 1\n",
        "            self.conv2_stride = stride\n",
        "        else:\n",
        "            self.conv1_stride = stride\n",
        "            self.conv2_stride = 1\n",
        "\n",
        "        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n",
        "        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n",
        "        self.norm3_name, norm3 = build_norm_layer(\n",
        "            norm_cfg, planes * self.expansion, postfix=3)\n",
        "\n",
        "        self.conv1 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            inplanes,\n",
        "            planes,\n",
        "            kernel_size=1,\n",
        "            stride=self.conv1_stride,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm1_name, norm1)\n",
        "        self.conv2 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            planes,\n",
        "            planes,\n",
        "            kernel_size=3,\n",
        "            stride=self.conv2_stride,\n",
        "            padding=dilation,\n",
        "            dilation=dilation,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm2_name, norm2)\n",
        "        self.conv3 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            planes,\n",
        "            planes * self.expansion,\n",
        "            kernel_size=1,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm3_name, norm3)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        return getattr(self, self.norm1_name)\n",
        "\n",
        "    @property\n",
        "    def norm2(self):\n",
        "        return getattr(self, self.norm2_name)\n",
        "\n",
        "    @property\n",
        "    def norm3(self):\n",
        "        return getattr(self, self.norm3_name)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        def _inner_forward(x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.norm1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.norm2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv3(out)\n",
        "            out = self.norm3(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "\n",
        "            out += identity\n",
        "\n",
        "            return out\n",
        "\n",
        "        if self.with_cp and x.requires_grad:\n",
        "            out = cp.checkpoint(_inner_forward, x)\n",
        "        else:\n",
        "            out = _inner_forward(x)\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def make_res_layer(block,\n",
        "                   inplanes,\n",
        "                   planes,\n",
        "                   blocks,\n",
        "                   stride=1,\n",
        "                   dilation=1,\n",
        "                   style='pytorch',\n",
        "                   with_cp=False,\n",
        "                   conv_cfg=None,\n",
        "                   norm_cfg=dict(type='BN')):\n",
        "    downsample = None\n",
        "    if stride != 1 or inplanes != planes * block.expansion:\n",
        "        downsample = nn.Sequential(\n",
        "            build_conv_layer(\n",
        "                conv_cfg,\n",
        "                inplanes,\n",
        "                planes * block.expansion,\n",
        "                kernel_size=1,\n",
        "                stride=stride,\n",
        "                bias=False),\n",
        "            build_norm_layer(norm_cfg, planes * block.expansion)[1],\n",
        "        )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(\n",
        "        block(\n",
        "            inplanes=inplanes,\n",
        "            planes=planes,\n",
        "            stride=stride,\n",
        "            dilation=dilation,\n",
        "            downsample=downsample,\n",
        "            style=style,\n",
        "            with_cp=with_cp,\n",
        "            conv_cfg=conv_cfg,\n",
        "            norm_cfg=norm_cfg))\n",
        "    inplanes = planes * block.expansion\n",
        "    for i in range(1, blocks):\n",
        "        layers.append(\n",
        "            block(\n",
        "                inplanes=inplanes,\n",
        "                planes=planes,\n",
        "                stride=1,\n",
        "                dilation=dilation,\n",
        "                style=style,\n",
        "                with_cp=with_cp,\n",
        "                conv_cfg=conv_cfg,\n",
        "                norm_cfg=norm_cfg))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet backbone.\n",
        "    Args:\n",
        "        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n",
        "        in_channels (int): Number of input image channels. Normally 3.\n",
        "        num_stages (int): Resnet stages, normally 4.\n",
        "        strides (Sequence[int]): Strides of the first block of each stage.\n",
        "        dilations (Sequence[int]): Dilation of each stage.\n",
        "        out_indices (Sequence[int]): Output from which stages.\n",
        "        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n",
        "            layer is the 3x3 conv layer, otherwise the stride-two layer is\n",
        "            the first 1x1 conv layer.\n",
        "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
        "            -1 means not freezing any parameters.\n",
        "        norm_cfg (dict): dictionary to construct and config norm layer.\n",
        "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
        "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
        "            and its variants only.\n",
        "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
        "            memory while slowing down the training speed.\n",
        "        zero_init_residual (bool): whether to use zero init for last norm layer\n",
        "            in resblocks to let them behave as identity.\n",
        "    Example:\n",
        "        >>> from openselfsup.models import ResNet\n",
        "        >>> import torch\n",
        "        >>> self = ResNet(depth=18)\n",
        "        >>> self.eval()\n",
        "        >>> inputs = torch.rand(1, 3, 32, 32)\n",
        "        >>> level_outputs = self.forward(inputs)\n",
        "        >>> for level_out in level_outputs:\n",
        "        ...     print(tuple(level_out.shape))\n",
        "        (1, 64, 8, 8)\n",
        "        (1, 128, 4, 4)\n",
        "        (1, 256, 2, 2)\n",
        "        (1, 512, 1, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    arch_settings = {\n",
        "        18: (BasicBlock, (2, 2, 2, 2)),\n",
        "        34: (BasicBlock, (3, 4, 6, 3)),\n",
        "        50: (Bottleneck, (3, 4, 6, 3)),\n",
        "        101: (Bottleneck, (3, 4, 23, 3)),\n",
        "        152: (Bottleneck, (3, 8, 36, 3))\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels=3,\n",
        "                 num_stages=4,\n",
        "                 strides=(1, 2, 2, 2),\n",
        "                 dilations=(1, 1, 1, 1),\n",
        "                 out_indices=(0, 1, 2, 3, 4),\n",
        "                 style='pytorch',\n",
        "                 frozen_stages=-1,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=dict(type='BN', requires_grad=True),\n",
        "                 norm_eval=False,\n",
        "                 with_cp=False,\n",
        "                 zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        if depth not in self.arch_settings:\n",
        "            raise KeyError('invalid depth {} for resnet'.format(depth))\n",
        "        self.depth = depth\n",
        "        self.num_stages = num_stages\n",
        "        assert num_stages >= 1 and num_stages <= 4\n",
        "        self.strides = strides\n",
        "        self.dilations = dilations\n",
        "        assert len(strides) == len(dilations) == num_stages\n",
        "        self.out_indices = out_indices\n",
        "        assert max(out_indices) < num_stages + 1\n",
        "        self.style = style\n",
        "        self.frozen_stages = frozen_stages\n",
        "        self.conv_cfg = conv_cfg\n",
        "        self.norm_cfg = norm_cfg\n",
        "        self.with_cp = with_cp\n",
        "        self.norm_eval = norm_eval\n",
        "        self.zero_init_residual = zero_init_residual\n",
        "        self.block, stage_blocks = self.arch_settings[depth]\n",
        "        self.stage_blocks = stage_blocks[:num_stages]\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self._make_stem_layer(in_channels)\n",
        "\n",
        "        self.res_layers = []\n",
        "        for i, num_blocks in enumerate(self.stage_blocks):\n",
        "            stride = strides[i]\n",
        "            dilation = dilations[i]\n",
        "            planes = 64 * 2 ** i\n",
        "            res_layer = make_res_layer(\n",
        "                self.block,\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                num_blocks,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                style=self.style,\n",
        "                with_cp=with_cp,\n",
        "                conv_cfg=conv_cfg,\n",
        "                norm_cfg=norm_cfg)\n",
        "            self.inplanes = planes * self.block.expansion\n",
        "            layer_name = 'layer{}'.format(i + 1)\n",
        "            self.add_module(layer_name, res_layer)\n",
        "            self.res_layers.append(layer_name)\n",
        "\n",
        "        self._freeze_stages()\n",
        "        self.feat_dims = [64]\n",
        "        self.feat_dims.extend([self.block.expansion * 64 * 2 ** (i) for i in range(len(self.stage_blocks))])\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        return getattr(self, self.norm1_name)\n",
        "\n",
        "    @property\n",
        "    def norm2(self):\n",
        "        return getattr(self, self.norm2_name)\n",
        "\n",
        "    def _make_stem_layer(self, in_channels):\n",
        "\n",
        "        self.conv1 = build_conv_layer(\n",
        "            self.conv_cfg,\n",
        "            in_channels,\n",
        "            64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, 64, postfix=1)\n",
        "        self.add_module(self.norm1_name, norm1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = build_conv_layer(\n",
        "            self.conv_cfg,\n",
        "            64,\n",
        "            64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.norm2_name, norm2 = build_norm_layer(self.norm_cfg, 64, postfix=2)\n",
        "        self.add_module(self.norm2_name, norm2)\n",
        "\n",
        "        self.stem = nn.Sequential(self.conv1, self.norm1, self.relu,\n",
        "                                  self.conv2, self.norm2, self.relu)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def _freeze_stages(self):\n",
        "        if self.frozen_stages >= 0:\n",
        "            self.norm1.eval()\n",
        "            for m in [self.conv1, self.norm1]:\n",
        "                for param in m.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        for i in range(1, self.frozen_stages + 1):\n",
        "            m = getattr(self, 'layer{}'.format(i))\n",
        "            m.eval()\n",
        "            for param in m.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        if isinstance(pretrained, str):\n",
        "            logger = get_root_logger()\n",
        "            load_checkpoint(self, pretrained, strict=False, logger=logger)\n",
        "        elif pretrained is None:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    kaiming_init(m, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n",
        "                    constant_init(m, 1)\n",
        "\n",
        "            if self.zero_init_residual:\n",
        "                for m in self.modules():\n",
        "                    if isinstance(m, Bottleneck):\n",
        "                        constant_init(m.norm3, 0)\n",
        "                    elif isinstance(m, BasicBlock):\n",
        "                        constant_init(m.norm2, 0)\n",
        "        else:\n",
        "            raise TypeError('pretrained must be a str or None')\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.stem(x)  # r50: 64x128x128\n",
        "        if 0 in self.out_indices:\n",
        "            outs.append(x)\n",
        "        x = self.maxpool(x)  # r50: 64x56x56\n",
        "        for i, layer_name in enumerate(self.res_layers):\n",
        "            res_layer = getattr(self, layer_name)\n",
        "            x = res_layer(x)\n",
        "            if i + 1 in self.out_indices:\n",
        "                outs.append(x)\n",
        "        # r50: 1-256x56x56; 2-512x28x28; 3-1024x14x14; 4-2048x7x7\n",
        "        return outs\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super(ResNet, self).train(mode)\n",
        "        self._freeze_stages()\n",
        "        if mode and self.norm_eval:\n",
        "            for m in self.modules():\n",
        "                # trick: eval have effect on BatchNorm only\n",
        "                if isinstance(m, _BatchNorm):\n",
        "                    m.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlqb9HAF8wm"
      },
      "source": [
        "## Center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5n9rQeoF8ww"
      },
      "outputs": [],
      "source": [
        "class MCTrans(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=240,\n",
        "                 nhead=8,\n",
        "                 d_ffn=1024,\n",
        "                 dropout=0.1,\n",
        "                 act=\"relu\",\n",
        "                 n_points=4,\n",
        "                 n_levels=3,\n",
        "                 n_sa_layers=6,\n",
        "                 in_channles=[64, 64, 128, 256, 512],\n",
        "                 proj_idxs=(2, 3, 4),\n",
        "\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.nhead = nhead\n",
        "        self.d_model = d_model\n",
        "        self.n_levels = n_levels\n",
        "\n",
        "        self.proj_idxs = proj_idxs\n",
        "        self.projs = nn.ModuleList()\n",
        "        for idx in self.proj_idxs:\n",
        "            self.projs.append(ConvModule(in_channles[idx],\n",
        "                                         d_model,\n",
        "                                         kernel_size=3,\n",
        "                                         padding=1,\n",
        "                                         conv_cfg=dict(type=\"Conv\"),\n",
        "                                         norm_cfg=dict(type='BN'),\n",
        "                                         act_cfg=dict(type='ReLU')\n",
        "                                         ))\n",
        "\n",
        "        dsa_layer = DSALayer(d_model=d_model,\n",
        "                             d_ffn=d_ffn,\n",
        "                             dropout=dropout,\n",
        "                             activation=act,\n",
        "                             n_levels=n_levels,\n",
        "                             n_heads=nhead,\n",
        "                             n_points=n_points)\n",
        "\n",
        "        self.dsa = DSA(att_layer=dsa_layer,\n",
        "                       n_layers=n_sa_layers)\n",
        "\n",
        "        self.level_embed = nn.Parameter(torch.Tensor(n_levels, d_model))\n",
        "        self.position_embedding = build_position_encoding(position_embedding=\"sine\", hidden_dim=d_model)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, MSDeformAttn):\n",
        "                m._reset_parameters()\n",
        "        normal_(self.level_embed)\n",
        "\n",
        "    def get_valid_ratio(self, mask):\n",
        "        _, H, W = mask.shape\n",
        "        valid_H = torch.sum(~mask[:, :, 0], 1)\n",
        "        valid_W = torch.sum(~mask[:, 0, :], 1)\n",
        "        valid_ratio_h = valid_H.float() / H\n",
        "        valid_ratio_w = valid_W.float() / W\n",
        "        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
        "        return valid_ratio\n",
        "\n",
        "    def projection(self, feats):\n",
        "        pos = []\n",
        "        masks = []\n",
        "        cnn_feats = []\n",
        "        tran_feats = []\n",
        "\n",
        "        for idx, feats in enumerate(feats):\n",
        "            if idx not in self.proj_idxs:\n",
        "                cnn_feats.append(feats)\n",
        "            else:\n",
        "                n, c, h, w = feats.shape\n",
        "                mask = torch.zeros((n, h, w)).to(torch.bool).to(feats.device)\n",
        "                nested_feats = NestedTensor(feats, mask)\n",
        "                masks.append(mask)\n",
        "                pos.append(self.position_embedding(nested_feats).to(nested_feats.tensors.dtype))\n",
        "                tran_feats.append(feats)\n",
        "\n",
        "        for idx, proj in enumerate(self.projs):\n",
        "            tran_feats[idx] = proj(tran_feats[idx])\n",
        "\n",
        "        return cnn_feats, tran_feats, pos, masks\n",
        "\n",
        "    def forward(self, x):\n",
        "        # project and prepare for the input\n",
        "        cnn_feats, trans_feats, pos_embs, masks = self.projection(x)\n",
        "        # dsa\n",
        "        features_flatten = []\n",
        "        mask_flatten = []\n",
        "        lvl_pos_embed_flatten = []\n",
        "        feature_shapes = []\n",
        "        spatial_shapes = []\n",
        "        for lvl, (feature, mask, pos_embed) in enumerate(zip(trans_feats, masks, pos_embs)):\n",
        "            bs, c, h, w = feature.shape\n",
        "            spatial_shapes.append((h, w))\n",
        "            feature_shapes.append(feature.shape)\n",
        "\n",
        "            feature = feature.flatten(2).transpose(1, 2)\n",
        "            mask = mask.flatten(1)\n",
        "            pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
        "            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n",
        "            lvl_pos_embed_flatten.append(lvl_pos_embed)\n",
        "\n",
        "            features_flatten.append(feature)\n",
        "            mask_flatten.append(mask)\n",
        "\n",
        "        features_flatten = torch.cat(features_flatten, 1)\n",
        "        mask_flatten = torch.cat(mask_flatten, 1)\n",
        "        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n",
        "        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=features_flatten.device)\n",
        "        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n",
        "        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n",
        "\n",
        "        # self att\n",
        "        feats = self.dsa(features_flatten,\n",
        "                         spatial_shapes,\n",
        "                         level_start_index,\n",
        "                         valid_ratios,\n",
        "                         lvl_pos_embed_flatten,\n",
        "                         mask_flatten)\n",
        "        # recover\n",
        "        out = []\n",
        "        features = feats.split(spatial_shapes.prod(1).tolist(), dim=1)\n",
        "        for idx, (feats, ori_shape) in enumerate(zip(features, spatial_shapes)):\n",
        "            out.append(feats.transpose(1, 2).reshape(feature_shapes[idx]))\n",
        "\n",
        "        cnn_feats.extend(out)\n",
        "        return cnn_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAP60gdBF8wp"
      },
      "outputs": [],
      "source": [
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
        "\n",
        "def build_position_encoding(position_embedding='sine', hidden_dim=240):\n",
        "    N_steps = hidden_dim // 2\n",
        "    if position_embedding in ('v2', 'sine'):\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif position_embedding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"not supported {position_embedding}\")\n",
        "\n",
        "    return position_embedding\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device, non_blocking=False):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device, non_blocking=non_blocking)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def record_stream(self, *args, **kwargs):\n",
        "        self.tensors.record_stream(*args, **kwargs)\n",
        "        if self.mask is not None:\n",
        "            self.mask.record_stream(*args, **kwargs)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 3) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG0RF0P6F8wr"
      },
      "outputs": [],
      "source": [
        "class MSDeformAttnFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step):\n",
        "        ctx.im2col_step = im2col_step\n",
        "        output = MSDA.ms_deform_attn_forward(\n",
        "            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, ctx.im2col_step)\n",
        "        ctx.save_for_backward(value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    @once_differentiable\n",
        "    def backward(ctx, grad_output):\n",
        "        value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights = ctx.saved_tensors\n",
        "        grad_value, grad_sampling_loc, grad_attn_weight = \\\n",
        "            MSDA.ms_deform_attn_backward(\n",
        "                value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, grad_output, ctx.im2col_step)\n",
        "\n",
        "        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxe2sBMF8ws"
      },
      "outputs": [],
      "source": [
        "def _is_power_of_2(n):\n",
        "    if (not isinstance(n, int)) or (n < 0):\n",
        "        raise ValueError(\"invalid input for _is_power_of_2: {} (type: {})\".format(n, type(n)))\n",
        "    return (n & (n-1) == 0) and n != 0\n",
        "\n",
        "class MSDeformAttn(nn.Module):\n",
        "    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n",
        "        \"\"\"\n",
        "        Multi-Scale Deformable Attention Module\n",
        "        :param d_model      hidden dimension\n",
        "        :param n_levels     number of feature levels\n",
        "        :param n_heads      number of attention heads\n",
        "        :param n_points     number of sampling points per attention head per feature level\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if d_model % n_heads != 0:\n",
        "            raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n",
        "        _d_per_head = d_model // n_heads\n",
        "        # you'd better set _d_per_head to a power of 2 which is more efficient in our CUDA implementation\n",
        "        if not _is_power_of_2(_d_per_head):\n",
        "            warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 \"\n",
        "                          \"which is more efficient in our CUDA implementation.\")\n",
        "\n",
        "        self.im2col_step = 64\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_levels = n_levels\n",
        "        self.n_heads = n_heads\n",
        "        self.n_points = n_points\n",
        "\n",
        "        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n",
        "        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n",
        "        self.value_proj = nn.Linear(d_model, d_model)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        constant_(self.sampling_offsets.weight.data, 0.)\n",
        "        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n",
        "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
        "        grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n",
        "        for i in range(self.n_points):\n",
        "            grid_init[:, :, i, :] *= i + 1\n",
        "        with torch.no_grad():\n",
        "            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n",
        "        constant_(self.attention_weights.weight.data, 0.)\n",
        "        constant_(self.attention_weights.bias.data, 0.)\n",
        "        xavier_uniform_(self.value_proj.weight.data)\n",
        "        constant_(self.value_proj.bias.data, 0.)\n",
        "        xavier_uniform_(self.output_proj.weight.data)\n",
        "        constant_(self.output_proj.bias.data, 0.)\n",
        "\n",
        "    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n",
        "        \"\"\"\n",
        "        :param query                       (N, Length_{query}, C)\n",
        "        :param reference_points            (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area\n",
        "                                        or (N, Length_{query}, n_levels, 4), add additional (w, h) to form reference boxes\n",
        "        :param input_flatten               (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l, C)\n",
        "        :param input_spatial_shapes        (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
        "        :param input_level_start_index     (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1, H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\n",
        "        :param input_padding_mask          (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l), True for padding elements, False for non-padding elements\n",
        "        :return output                     (N, Length_{query}, C)\n",
        "        \"\"\"\n",
        "        N, Len_q, _ = query.shape\n",
        "        N, Len_in, _ = input_flatten.shape\n",
        "        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
        "\n",
        "        value = self.value_proj(input_flatten)\n",
        "        if input_padding_mask is not None:\n",
        "            value = value.masked_fill(input_padding_mask[..., None], float(0))\n",
        "        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n",
        "        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n",
        "        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n",
        "        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n",
        "        # N, Len_q, n_heads, n_levels, n_points, 2\n",
        "        if reference_points.shape[-1] == 2:\n",
        "            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n",
        "            sampling_locations = reference_points[:, :, None, :, None, :] \\\n",
        "                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n",
        "        elif reference_points.shape[-1] == 4:\n",
        "            sampling_locations = reference_points[:, :, None, :, None, :2] \\\n",
        "                                 + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n",
        "        output = MSDeformAttnFunction.apply(\n",
        "            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n",
        "        output = self.output_proj(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qQ0DUHlF8wu"
      },
      "outputs": [],
      "source": [
        "class DSALayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=256, d_ffn=1024,\n",
        "                 dropout=0.1, activation=\"relu\",\n",
        "                 n_levels=4, n_heads=8, n_points=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # self attention\n",
        "        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # ffn\n",
        "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def with_pos_embed(tensor, pos):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_ffn(self, src):\n",
        "        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout3(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n",
        "        # self attention\n",
        "        src2 = self.self_attn(self.with_pos_embed(src, pos),\n",
        "                              reference_points,\n",
        "                              src,\n",
        "                              spatial_shapes,\n",
        "                              level_start_index,\n",
        "                              padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        # ffn\n",
        "        src = self.forward_ffn(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class DSA(nn.Module):\n",
        "    def __init__(self, att_layer, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(att_layer, n_layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
        "        reference_points_list = []\n",
        "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
        "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
        "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
        "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
        "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
        "            ref = torch.stack((ref_x, ref_y), -1)\n",
        "            reference_points_list.append(ref)\n",
        "        reference_points = torch.cat(reference_points_list, 1)\n",
        "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
        "        return reference_points\n",
        "\n",
        "    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n",
        "        output = src\n",
        "        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n",
        "        for _, layer in enumerate(self.layers):\n",
        "            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ffn=1024,\n",
        "                 dropout=0.1, activation=\"relu\", n_heads=8):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # self attention\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # ffn\n",
        "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
        "        self.dropout4 = nn.Dropout(dropout)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def with_pos_embed(tensor, pos):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_ffn(self, tgt):\n",
        "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout4(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        # self attention\n",
        "        q = k = tgt\n",
        "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        # cross attention\n",
        "        tgt2 = self.cross_attn(tgt, src, src)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        # ffn\n",
        "        tgt = self.forward_ffn(tgt)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class CA(nn.Module):\n",
        "    def __init__(self, att_layer, n_layers, n_category=2, d_model=256):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(att_layer, n_layers)\n",
        "        self.proxy_embed = nn.Parameter(torch.zeros(1, n_category, d_model))\n",
        "\n",
        "    def forward(self, src):\n",
        "        query = None\n",
        "        B = src.shape[0]\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if idx == 0:\n",
        "                query = self.proxy_embed.expand(B, -1, -1)\n",
        "            else:\n",
        "                query += self.proxy_embed.expand(B, -1, -1)\n",
        "            query = layer(query, src)\n",
        "        return query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUn-TZURGKyg"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg0et0_wGOg6"
      },
      "outputs": [],
      "source": [
        "def conv_bn_relu(in_channels, out_channels, kernel_size=3, padding=1, stride=1):\n",
        "    return nn.Sequential(nn.Conv2d(in_channels,\n",
        "                                   out_channels,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   padding=padding,\n",
        "                                   stride=stride),\n",
        "                         nn.BatchNorm2d(out_channels),\n",
        "                         nn.ReLU(inplace=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Sx8JNOoGW_R"
      },
      "outputs": [],
      "source": [
        "class AttBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=F_g,\n",
        "                      out_channels=F_int,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=F_l,\n",
        "                      out_channels=F_int,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=F_int,\n",
        "                      out_channels=1,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return x * psi\n",
        "\n",
        "\n",
        "class DecBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            skip_channels,\n",
        "            out_channels,\n",
        "            attention=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv_bn_relu(in_channels=in_channels + skip_channels,\n",
        "                                  out_channels=out_channels)\n",
        "\n",
        "        self.conv2 = conv_bn_relu(in_channels=out_channels,\n",
        "                                  out_channels=out_channels)\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2,\n",
        "                              mode='bilinear',\n",
        "                              align_corners=True)\n",
        "\n",
        "        if attention:\n",
        "            self.att = AttBlock(F_g=in_channels, F_l=skip_channels, F_int=in_channels)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.up(x)\n",
        "        if skip is not None:\n",
        "            if hasattr(self, \"att\"):\n",
        "                skip = self.att(g=x, x=skip)\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class UNetDecoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            att=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        in_channels = in_channels[::-1]\n",
        "        skip_channels = in_channels[1:]\n",
        "        for in_c, skip_c in zip(in_channels, skip_channels):\n",
        "            self.decoders.append(DecBlock(in_c, skip_c, skip_c, att))\n",
        "\n",
        "    def forward(self, features):\n",
        "        features = features[::-1]\n",
        "        x = features[0]\n",
        "        skips = features[1:]\n",
        "\n",
        "        for i, layer in enumerate(self.decoders):\n",
        "            x = layer(x, skips[i])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYhyXm1BGqlR"
      },
      "source": [
        "## Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylm3QwCpG0aB"
      },
      "outputs": [],
      "source": [
        "class MCTransAuxHead(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=240,\n",
        "                 d_ffn=1024,\n",
        "                 dropout=0.1,\n",
        "                 act=\"relu\",\n",
        "                 n_head=8,\n",
        "                 n_layers=4,\n",
        "                 num_classes=1,\n",
        "                 in_channles=[64, 64, 128, 256, 512],\n",
        "                 proj_idxs=(2, 3, 4),\n",
        "                 losses=None\n",
        "                 ):\n",
        "        super(MCTransAuxHead, self).__init__()\n",
        "        self.in_channles = in_channles\n",
        "        self.proj_idxs = proj_idxs\n",
        "\n",
        "        ca_layer = CALayer(d_model=d_model,\n",
        "                           d_ffn=d_ffn,\n",
        "                           dropout=dropout,\n",
        "                           activation=act,\n",
        "                           n_heads=n_head)\n",
        "\n",
        "        self.ca = CA(att_layer=ca_layer,\n",
        "                     n_layers=n_layers,\n",
        "                     n_category=num_classes,\n",
        "                     d_model=d_model)\n",
        "\n",
        "        self.head = nn.Sequential(nn.Linear(num_classes*d_model, d_model),\n",
        "                                  nn.Linear(d_model, num_classes))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # flatten\n",
        "        inputs = [inputs[idx] for idx in self.proj_idxs]\n",
        "        inputs_flatten = [item.flatten(2).transpose(1, 2) for item in inputs]\n",
        "        inputs_flatten = torch.cat(inputs_flatten, 1)\n",
        "        # ca\n",
        "        outputs = self.ca(inputs_flatten)\n",
        "        logits = self.head(outputs.flatten(1))\n",
        "       \n",
        "        return logits\n",
        "\n",
        "    def init_weights(self):\n",
        "        normal_init(self.head, mean=0, std=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIv-NOL1XLih"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "-MeFJOELXNpe"
      },
      "outputs": [],
      "source": [
        "class BinaryFocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
        "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
        "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
        "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
        "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
        "                    focus on hard misclassified example\n",
        "    :param reduction: `none`|`mean`|`sum`\n",
        "    :param **kwargs\n",
        "        balance_index: (int) balance class index, should be specific when alpha is float\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=3, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n",
        "        super(BinaryFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.smooth = 1e-6  # set '1e-4' when train with FP16\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "        assert self.reduction in ['none', 'mean', 'sum']\n",
        "\n",
        "        # if self.alpha is None:\n",
        "        #     self.alpha = torch.ones(2)\n",
        "        # elif isinstance(self.alpha, (list, np.ndarray)):\n",
        "        #     self.alpha = np.asarray(self.alpha)\n",
        "        #     self.alpha = np.reshape(self.alpha, (2))\n",
        "        #     assert self.alpha.shape[0] == 2, \\\n",
        "        #         'the `alpha` shape is not match the number of class'\n",
        "        # elif isinstance(self.alpha, (float, int)):\n",
        "        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n",
        "\n",
        "        # else:\n",
        "        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        prob = torch.sigmoid(output)\n",
        "        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n",
        "\n",
        "        valid_mask = None\n",
        "        if self.ignore_index is not None:\n",
        "            valid_mask = (target != self.ignore_index).float()\n",
        "\n",
        "        pos_mask = (target == 1).float()\n",
        "        neg_mask = (target == 0).float()\n",
        "        if valid_mask is not None:\n",
        "            pos_mask = pos_mask * valid_mask\n",
        "            neg_mask = neg_mask * valid_mask\n",
        "\n",
        "        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n",
        "        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n",
        "\n",
        "        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n",
        "        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss = loss.mean()\n",
        "        return loss\n",
        "\n",
        "def reduce_loss(loss, reduction):\n",
        "    \"\"\"Reduce loss as specified.\n",
        "    Args:\n",
        "        loss (Tensor): Elementwise loss tensor.\n",
        "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
        "    Return:\n",
        "        Tensor: Reduced loss tensor.\n",
        "    \"\"\"\n",
        "    reduction_enum = F._Reduction.get_enum(reduction)\n",
        "    # none: 0, elementwise_mean:1, sum: 2\n",
        "    if reduction_enum == 0:\n",
        "        return loss\n",
        "    elif reduction_enum == 1:\n",
        "        return loss.mean()\n",
        "    elif reduction_enum == 2:\n",
        "        return loss.sum()\n",
        "\n",
        "\n",
        "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
        "    \"\"\"Apply element-wise weight and reduce loss.\n",
        "    Args:\n",
        "        loss (Tensor): Element-wise loss.\n",
        "        weight (Tensor): Element-wise weights.\n",
        "        reduction (str): Same as built-in losses of PyTorch.\n",
        "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
        "    Returns:\n",
        "        Tensor: Processed loss values.\n",
        "    \"\"\"\n",
        "    # if weight is specified, apply element-wise weight\n",
        "    if weight is not None:\n",
        "        loss = loss * weight\n",
        "\n",
        "    # if avg_factor is not specified, just reduce the loss\n",
        "    if avg_factor is None:\n",
        "        loss = reduce_loss(loss, reduction)\n",
        "    else:\n",
        "        # if reduction is mean, then average the loss by avg_factor\n",
        "        if reduction == 'mean':\n",
        "            loss = loss.sum() / avg_factor\n",
        "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
        "        elif reduction != 'none':\n",
        "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):\n",
        "    \"\"\"Calculate the CrossEntropy loss.\n",
        "    Args:\n",
        "        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n",
        "            of classes.\n",
        "        label (torch.Tensor): The gt label of the prediction.\n",
        "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
        "        reduction (str): The method used to reduce the loss.\n",
        "        avg_factor (int, optional): Average factor that is used to average\n",
        "            the loss. Defaults to None.\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated loss\n",
        "    \"\"\"\n",
        "    # element-wise losses\n",
        "    loss = F.cross_entropy(pred, label, reduction='none')\n",
        "\n",
        "    # apply weights and do the reduction\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "    loss = weight_reduce_loss(\n",
        "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def binary_cross_entropy(pred,\n",
        "                         label,\n",
        "                         weight=None,\n",
        "                         reduction='mean',\n",
        "                         avg_factor=None):\n",
        "    \"\"\"Calculate the binary CrossEntropy loss with logits.\n",
        "    Args:\n",
        "        pred (torch.Tensor): The prediction with shape (N, *).\n",
        "        label (torch.Tensor): The gt label with shape (N, *).\n",
        "        weight (torch.Tensor, optional): Element-wise weight of loss with shape\n",
        "             (N, ). Defaults to None.\n",
        "        reduction (str): The method used to reduce the loss.\n",
        "            Options are \"none\", \"mean\" and \"sum\". If reduction is 'none' , loss\n",
        "             is same shape as pred and label. Defaults to 'mean'.\n",
        "        avg_factor (int, optional): Average factor that is used to average\n",
        "            the loss. Defaults to None.\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated loss\n",
        "    \"\"\"\n",
        "    # assert pred.dim() == label.dim()\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(pred, label, reduction='none')\n",
        "\n",
        "    # apply weights and do the reduction\n",
        "    if weight is not None:\n",
        "        assert weight.dim() == 1\n",
        "        weight = weight.float()\n",
        "        if pred.dim() > 1:\n",
        "            weight = weight.reshape(-1, 1)\n",
        "    loss = weight_reduce_loss(\n",
        "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
        "    return loss\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "    \"\"\"Cross entropy loss.\n",
        "    Args:\n",
        "        use_sigmoid (bool): Whether the prediction uses sigmoid\n",
        "            of softmax. Defaults to False.\n",
        "        use_soft (bool): Whether to use the soft version of CrossEntropyLoss.\n",
        "            Defaults to False.\n",
        "        reduction (str): The method used to reduce the loss.\n",
        "            Options are \"none\", \"mean\" and \"sum\". Defaults to 'mean'.\n",
        "        loss_weight (float):  Weight of the loss. Defaults to 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sigmoid=False,\n",
        "                 softmax=False,\n",
        "                 reduction='mean',\n",
        "                 loss_weight=1.0):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        self.use_sigmoid = sigmoid\n",
        "        self.use_soft = softmax\n",
        "        assert not (\n",
        "                self.use_soft and self.use_sigmoid\n",
        "        ), 'use_sigmoid and use_soft could not be set simultaneously'\n",
        "\n",
        "        self.reduction = reduction\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "        if self.use_sigmoid:\n",
        "            self.cls_criterion = binary_cross_entropy\n",
        "        elif self.use_soft:\n",
        "            self.cls_criterion = soft_cross_entropy\n",
        "        else:\n",
        "            self.cls_criterion = cross_entropy\n",
        "\n",
        "    def forward(self,\n",
        "                cls_score,\n",
        "                label,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "\n",
        "        n_pred_ch, n_target_ch = cls_score.shape[1], label.shape[1]\n",
        "        if n_pred_ch == n_target_ch:\n",
        "            label = torch.argmax(label, dim=1)\n",
        "        else:\n",
        "            label = torch.squeeze(label, dim=1)\n",
        "        label = label.long()\n",
        "\n",
        "        loss_cls = self.loss_weight * self.cls_criterion(\n",
        "            cls_score,\n",
        "            label,\n",
        "            weight,\n",
        "            reduction=reduction,\n",
        "            avg_factor=avg_factor,\n",
        "            **kwargs)\n",
        "        return loss_cls\n",
        "\n",
        "\n",
        "class MCTransAuxLoss(CrossEntropyLoss):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(MCTransAuxLoss, self).__init__(**kwargs)\n",
        "\n",
        "    def forward(self,\n",
        "                cls_score,\n",
        "                label,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        #To one hot\n",
        "        num_classes = cls_score.shape[1]\n",
        "        one_hot_list = []\n",
        "\n",
        "        for l in label:\n",
        "            one_hot_list.append(self.one_hot(torch.unique(l), num_classes=num_classes).sum(dim=0))\n",
        "        label = torch.stack(one_hot_list) - 1\n",
        "\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "        \n",
        "        loss_cls = self.loss_weight * self.cls_criterion(\n",
        "            cls_score,\n",
        "            label,\n",
        "            weight,\n",
        "            reduction=reduction,\n",
        "            avg_factor=avg_factor,\n",
        "            **kwargs)\n",
        "\n",
        "        return loss_cls\n",
        "\n",
        "    def one_hot(self, input, num_classes, dtype=torch.float):\n",
        "        assert input.dim() > 0, \"input should have dim of 1 or more.\"\n",
        "\n",
        "        # if 1D, add singelton dim at the end\n",
        "        if input.dim() == 1:\n",
        "            input = input.view(-1, 1)\n",
        "\n",
        "        sh = list(input.shape)\n",
        "\n",
        "        assert sh[1] == 1, \"labels should have a channel with length equals to one.\"\n",
        "        sh[1] = num_classes\n",
        "\n",
        "        o = torch.zeros(size=sh, dtype=dtype, device=input.device)\n",
        "        labels = o.scatter_(dim=0, index=input.long(), value=1)\n",
        "        return labels\n",
        "\n",
        "class VolumeLoss(nn.Module):\n",
        "    def __init__(self, threshold=0.25):\n",
        "        super(VolumeLoss, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        output[output > self.threshold] = 1.0\n",
        "        output[output <= self.threshold] = 0.0\n",
        "\n",
        "        vol_output = output.sum()/1000\n",
        "        target_output = target.sum()/1000\n",
        "        vol_loss = torch.abs(target_output-vol_output)\n",
        "        return vol_loss\n",
        "\n",
        "class TotalLoss(nn.Module):\n",
        "    def __init__(self, alpha=3, gamma=2, threshold=0.25):\n",
        "        super(TotalLoss, self).__init__()\n",
        "        self.focal_loss = BinaryFocalLoss(alpha=alpha, gamma=gamma)\n",
        "        self.volume_loss = VolumeLoss(threshold=threshold) \n",
        "        self.aux_loss = MCTransAuxLoss(sigmoid=True, loss_weight=0.1)\n",
        "\n",
        "    def forward(self, output, logits, target):\n",
        "        fl = self.focal_loss(output, target)\n",
        "        vl = self.volume_loss(output, target)\n",
        "        al = self.aux_loss(logits, target)\n",
        "\n",
        "        total_loss = fl + al #+ vl\n",
        "        return fl, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTRdXaEC8x0"
      },
      "source": [
        "# 4.3. K-Fold (pytorch model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrLlkkca0pCc"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhYz5uvODIUs",
        "outputId": "dbb16a04-2c5c-4956-f140-405a3bd7dffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t Focal Loss: 0.015879649142550453 \t Total Loss: 0.08726239156165569\t -Val Focal Loss: 3.4807965026370115e-05 \t Val Total Loss: 0.0016523749700614384\n",
            "Epoch 2 \t Focal Loss: 0.002611007558587242 \t Total Loss: 0.0728116759748982\t -Val Focal Loss: 1.288007728622428e-05 \t Val Total Loss: 0.0019154870084353856\n",
            "Epoch 3 \t Focal Loss: 0.001662268081025418 \t Total Loss: 0.07163788598004005\t -Val Focal Loss: 9.386359181787287e-05 \t Val Total Loss: 0.001549223278250013\n",
            "Epoch 4 \t Focal Loss: 0.0015154593956984224 \t Total Loss: 0.07123752686081172\t -Val Focal Loss: 2.617793091173683e-06 \t Val Total Loss: 0.001996400100844247\n",
            "Epoch 5 \t Focal Loss: 0.0013339648019792948 \t Total Loss: 0.07089892400951266\t -Val Focal Loss: 2.2051533700765245e-06 \t Val Total Loss: 0.0019582054444721767\n",
            "Epoch 6 \t Focal Loss: 0.0011802909323856026 \t Total Loss: 0.07072532613822025\t -Val Focal Loss: 3.878370625898242e-06 \t Val Total Loss: 0.0018223941326141358\n",
            "Epoch 7 \t Focal Loss: 0.00117163595446367 \t Total Loss: 0.07064199225186445\t -Val Focal Loss: 1.0826072996548775e-06 \t Val Total Loss: 0.0016821154526301793\n",
            "Epoch 8 \t Focal Loss: 0.0011361491819403952 \t Total Loss: 0.07064837297732882\t -Val Focal Loss: 9.547008500833597e-07 \t Val Total Loss: 0.0018892503210476467\n",
            "Epoch 9 \t Focal Loss: 0.0009848124996260463 \t Total Loss: 0.07049471661180472\t -Val Focal Loss: 5.526107997866347e-07 \t Val Total Loss: 0.0021062018615858897\n",
            "Epoch 10 \t Focal Loss: 0.0008683165537241366 \t Total Loss: 0.07032447796073749\t -Val Focal Loss: 4.6755532301696285e-07 \t Val Total Loss: 0.00216473468712398\n",
            "Epoch 11 \t Focal Loss: 0.000834769443768211 \t Total Loss: 0.07029420491173971\t -Val Focal Loss: 3.655551544008111e-07 \t Val Total Loss: 0.0018658821071897234\n",
            "Epoch 12 \t Focal Loss: 0.0009176239641583248 \t Total Loss: 0.07030896405629117\t -Val Focal Loss: 4.057618623067226e-07 \t Val Total Loss: 0.0019806666033608572\n",
            "Epoch 13 \t Focal Loss: 0.00082240766441697 \t Total Loss: 0.07024528845983873\t -Val Focal Loss: 2.433710635939081e-07 \t Val Total Loss: 0.0019673119698251995\n",
            "Epoch 14 \t Focal Loss: 0.0007903015124572297 \t Total Loss: 0.07015416272681393\t -Val Focal Loss: 1.8509587919522475e-07 \t Val Total Loss: 0.0017706056790692465\n",
            "Epoch 15 \t Focal Loss: 0.0007854929938815553 \t Total Loss: 0.07034458575113643\t -Val Focal Loss: 1.9636779273111772e-07 \t Val Total Loss: 0.0019050600273268564\n",
            "Epoch 16 \t Focal Loss: 0.00080270241056928 \t Total Loss: 0.07032180195607299\t -Val Focal Loss: 2.6405624729315084e-07 \t Val Total Loss: 0.0018199782286371503\n",
            "Epoch 17 \t Focal Loss: 0.0007133257907351029 \t Total Loss: 0.07008515808918922\t -Val Focal Loss: 1.7105520393150592e-07 \t Val Total Loss: 0.0018920070358685085\n",
            "Epoch 18 \t Focal Loss: 0.0007036401343611942 \t Total Loss: 0.07007670145240619\t -Val Focal Loss: 1.7998306377973805e-07 \t Val Total Loss: 0.0018535232969692774\n",
            "Epoch 19 \t Focal Loss: 0.0007422189515643544 \t Total Loss: 0.0700974531418128\t -Val Focal Loss: 8.438135929671781e-08 \t Val Total Loss: 0.0015644750424793788\n",
            "Epoch 20 \t Focal Loss: 0.0007137040613673784 \t Total Loss: 0.07000591859167857\t -Val Focal Loss: 1.0542976919428578e-07 \t Val Total Loss: 0.0021534123591014317\n",
            "Epoch 21 \t Focal Loss: 0.0006525500663536791 \t Total Loss: 0.0700713954061913\t -Val Focal Loss: 6.957231042698757e-08 \t Val Total Loss: 0.0019131498677389962\n",
            "Epoch 22 \t Focal Loss: 0.0006480598965317899 \t Total Loss: 0.07002388379074258\t -Val Focal Loss: 8.36276415481864e-08 \t Val Total Loss: 0.0019817354423659187\n",
            "Epoch 23 \t Focal Loss: 0.0006301638895992118 \t Total Loss: 0.06997124537182368\t -Val Focal Loss: 4.764228833664674e-08 \t Val Total Loss: 0.001905337827546256\n",
            "Epoch 24 \t Focal Loss: 0.000597909770499591 \t Total Loss: 0.07000384434200019\t -Val Focal Loss: 9.95486061583506e-08 \t Val Total Loss: 0.0019113930208342416\n",
            "Epoch 25 \t Focal Loss: 0.0006846403912849834 \t Total Loss: 0.06999768746896184\t -Val Focal Loss: 6.008487746709891e-08 \t Val Total Loss: 0.0019351733582360403\n",
            "Epoch 26 \t Focal Loss: 0.0005903262594738604 \t Total Loss: 0.06993078599891646\t -Val Focal Loss: 7.358053153438959e-08 \t Val Total Loss: 0.0018968303288732257\n",
            "Epoch 27 \t Focal Loss: 0.0005822856622518733 \t Total Loss: 0.06990253113263803\t -Val Focal Loss: 3.671584961141759e-08 \t Val Total Loss: 0.001740824111870357\n",
            "Epoch 28 \t Focal Loss: 0.0005802193029728887 \t Total Loss: 0.06991897912012587\t -Val Focal Loss: 5.254002449614095e-08 \t Val Total Loss: 0.0017515134598527636\n",
            "Epoch 29 \t Focal Loss: 0.000593282558703067 \t Total Loss: 0.06993687135442127\t -Val Focal Loss: 3.241845791178223e-08 \t Val Total Loss: 0.0018704855016299657\n",
            "Epoch 30 \t Focal Loss: 0.0006503098048374701 \t Total Loss: 0.07000447005676709\t -Val Focal Loss: 1.8153400560549926e-08 \t Val Total Loss: 0.0018730736204556056\n",
            "Epoch 31 \t Focal Loss: 0.0005565051738075316 \t Total Loss: 0.06990797538980306\t -Val Focal Loss: 2.3311802773865305e-08 \t Val Total Loss: 0.0018964671662875584\n",
            "Epoch 32 \t Focal Loss: 0.0005388824110293675 \t Total Loss: 0.06988373047692313\t -Val Focal Loss: 1.709596517684986e-08 \t Val Total Loss: 0.0015519031456538608\n",
            "Epoch 33 \t Focal Loss: 0.0005089017969049342 \t Total Loss: 0.07000719458400774\t -Val Focal Loss: 4.20977130488609e-08 \t Val Total Loss: 0.001859439696584429\n",
            "Epoch 34 \t Focal Loss: 0.0005473292257444044 \t Total Loss: 0.06991384081917701\t -Val Focal Loss: 1.7441108671586594e-08 \t Val Total Loss: 0.0018724467073168074\n",
            "Epoch 35 \t Focal Loss: 0.0005493531793863505 \t Total Loss: 0.06992608741622605\t -Val Focal Loss: 2.4691792077646822e-08 \t Val Total Loss: 0.001852633271898542\n",
            "Epoch 36 \t Focal Loss: 0.0005467019370169807 \t Total Loss: 0.06988396415178724\t -Val Focal Loss: 2.0994879637977905e-08 \t Val Total Loss: 0.0018117208565984453\n",
            "Epoch 37 \t Focal Loss: 0.0005176873917910944 \t Total Loss: 0.069958419853942\t -Val Focal Loss: 2.125630252131876e-08 \t Val Total Loss: 0.0018816720162119183\n",
            "Epoch 38 \t Focal Loss: 0.0005031336303081124 \t Total Loss: 0.0698319983487721\t -Val Focal Loss: 9.831406292245707e-09 \t Val Total Loss: 0.001878124475479126\n",
            "Epoch 39 \t Focal Loss: 0.0005259791252782464 \t Total Loss: 0.06990549993439973\t -Val Focal Loss: 1.8181203488763588e-08 \t Val Total Loss: 0.0017787603395325797\n",
            "Epoch 40 \t Focal Loss: 0.00048259367196921445 \t Total Loss: 0.06982201695603003\t -Val Focal Loss: 8.5359528968963e-09 \t Val Total Loss: 0.0018996406878743852\n",
            "Epoch 41 \t Focal Loss: 0.0004966448913489049 \t Total Loss: 0.06977101230471254\t -Val Focal Loss: 1.2656202151057575e-07 \t Val Total Loss: 0.0018551234688077654\n",
            "Epoch 42 \t Focal Loss: 0.0004761997859165306 \t Total Loss: 0.06980398948565661\t -Val Focal Loss: 2.1989451072710967e-08 \t Val Total Loss: 0.0017912641167640686\n",
            "Epoch 43 \t Focal Loss: 0.0005151131539844383 \t Total Loss: 0.06980194679374317\t -Val Focal Loss: 1.3691185033946697e-07 \t Val Total Loss: 0.0017962517482893808\n",
            "Epoch 44 \t Focal Loss: 0.0005035879171111655 \t Total Loss: 0.06986307870033834\t -Val Focal Loss: 1.7948316001690858e-08 \t Val Total Loss: 0.0017335438302585058\n",
            "Epoch 45 \t Focal Loss: 0.00047606258475742203 \t Total Loss: 0.06975923867212783\t -Val Focal Loss: 1.7177933386847145e-08 \t Val Total Loss: 0.0019052878022193908\n",
            "Epoch 46 \t Focal Loss: 0.00044488728870336827 \t Total Loss: 0.06977491799018366\t -Val Focal Loss: 6.588392206400126e-09 \t Val Total Loss: 0.0018431169646126882\n",
            "Epoch 47 \t Focal Loss: 0.000432649537018244 \t Total Loss: 0.06968982163950693\t -Val Focal Loss: 1.0220386619559057e-08 \t Val Total Loss: 0.002015716050352369\n",
            "Epoch 48 \t Focal Loss: 0.0004553795128370941 \t Total Loss: 0.06979193660370309\t -Val Focal Loss: 2.955045731921148e-08 \t Val Total Loss: 0.0018943220376968384\n",
            "Epoch 49 \t Focal Loss: 0.0004492764497324711 \t Total Loss: 0.069728538876386\t -Val Focal Loss: 7.870465066974117e-09 \t Val Total Loss: 0.001793330694947924\n",
            "Epoch 50 \t Focal Loss: 0.0005532278616877981 \t Total Loss: 0.0698367724288925\t -Val Focal Loss: 1.3148202729748196e-08 \t Val Total Loss: 0.0018833241292408535\n",
            "Epoch 51 \t Focal Loss: 0.0004548518971024586 \t Total Loss: 0.06980341329849024\t -Val Focal Loss: 2.4116851753725705e-09 \t Val Total Loss: 0.0018114430563790458\n",
            "Epoch 52 \t Focal Loss: 0.00044079808181244213 \t Total Loss: 0.06967102511109208\t -Val Focal Loss: 1.5070405368078355e-09 \t Val Total Loss: 0.0018589096409933907\n",
            "Epoch 53 \t Focal Loss: 0.00046344003161876037 \t Total Loss: 0.06974025180549931\t -Val Focal Loss: 7.4120204577151785e-09 \t Val Total Loss: 0.0018547002758298602\n",
            "Epoch 54 \t Focal Loss: 0.000406774445179521 \t Total Loss: 0.06968780325578271\t -Val Focal Loss: 2.0741867144481926e-09 \t Val Total Loss: 0.0018656573125294276\n",
            "Epoch 55 \t Focal Loss: 0.00039834438796345684 \t Total Loss: 0.06972564754190205\t -Val Focal Loss: 1.419682961665915e-09 \t Val Total Loss: 0.0018121996096202306\n",
            "Epoch 56 \t Focal Loss: 0.0005307360729952454 \t Total Loss: 0.06987144129364181\t -Val Focal Loss: 2.5684864470219638e-08 \t Val Total Loss: 0.0018184666122709001\n",
            "Epoch 57 \t Focal Loss: 0.0004294129419856385 \t Total Loss: 0.06969608504512756\t -Val Focal Loss: 2.891414599811729e-09 \t Val Total Loss: 0.0019004338553973606\n",
            "Epoch 58 \t Focal Loss: 0.0004104036867092235 \t Total Loss: 0.06973064751129784\t -Val Focal Loss: 3.1987350926751136e-09 \t Val Total Loss: 0.001841773305620466\n",
            "Epoch 59 \t Focal Loss: 0.00040636958338800453 \t Total Loss: 0.0696992309991833\t -Val Focal Loss: 2.872028963858091e-09 \t Val Total Loss: 0.0018172259841646467\n",
            "Epoch 60 \t Focal Loss: 0.00041012611783818634 \t Total Loss: 0.06968786466947134\t -Val Focal Loss: 1.0270601892768485e-09 \t Val Total Loss: 0.0018177262374332974\n",
            "Epoch 61 \t Focal Loss: 0.00038367474606989025 \t Total Loss: 0.06963940858572935\t -Val Focal Loss: 7.405112967262539e-10 \t Val Total Loss: 0.001794985362461635\n",
            "Epoch 62 \t Focal Loss: 0.0005661631278937695 \t Total Loss: 0.06989787125008569\t -Val Focal Loss: 9.081940025420668e-10 \t Val Total Loss: 0.0018337669117110117\n",
            "Epoch 63 \t Focal Loss: 0.000456854032870023 \t Total Loss: 0.0697417222231412\t -Val Focal Loss: 2.356050288199185e-09 \t Val Total Loss: 0.001911927546773638\n",
            "Epoch 64 \t Focal Loss: 0.0003938487148718364 \t Total Loss: 0.06972273784897311\t -Val Focal Loss: 4.510560813741904e-10 \t Val Total Loss: 0.0021505707076617648\n",
            "Epoch 65 \t Focal Loss: 0.00037933374908043763 \t Total Loss: 0.06975646775296267\t -Val Focal Loss: 1.6885603518598405e-09 \t Val Total Loss: 0.0018757758396012443\n",
            "Epoch 66 \t Focal Loss: 0.00037554743190715436 \t Total Loss: 0.06961538540802413\t -Val Focal Loss: 1.2745401940199893e-09 \t Val Total Loss: 0.001856969084058489\n",
            "Epoch 67 \t Focal Loss: 0.00037116202438303077 \t Total Loss: 0.06964682011777977\t -Val Focal Loss: 7.365473818156586e-10 \t Val Total Loss: 0.0018283107451030187\n",
            "Epoch 68 \t Focal Loss: 0.000375907728143396 \t Total Loss: 0.06963631915424368\t -Val Focal Loss: 1.4219636008354948e-09 \t Val Total Loss: 0.0018474489450454711\n",
            "Epoch 69 \t Focal Loss: 0.0003660277543327048 \t Total Loss: 0.06965975058700541\t -Val Focal Loss: 8.440676221458228e-10 \t Val Total Loss: 0.0018483338611466544\n",
            "Epoch 70 \t Focal Loss: 0.00038249703208743225 \t Total Loss: 0.06961489029282289\t -Val Focal Loss: 2.522461731554618e-10 \t Val Total Loss: 0.0018007557306970868\n",
            "Epoch 71 \t Focal Loss: 0.00034812614561213724 \t Total Loss: 0.06963449522263283\t -Val Focal Loss: 1.162940626337721e-09 \t Val Total Loss: 0.001845574378967285\n",
            "Epoch 72 \t Focal Loss: 0.0003390066644768854 \t Total Loss: 0.0696465020610703\t -Val Focal Loss: 5.750255890494113e-10 \t Val Total Loss: 0.001852825709751674\n",
            "Epoch 73 \t Focal Loss: 0.00035396679273378003 \t Total Loss: 0.06961597654995301\t -Val Focal Loss: 1.9584858179655384e-10 \t Val Total Loss: 0.001884530484676361\n",
            "Epoch 74 \t Focal Loss: 0.00035287088743397036 \t Total Loss: 0.06956557456484373\t -Val Focal Loss: 2.1325496289494236e-10 \t Val Total Loss: 0.002027214424950736\n",
            "Epoch 75 \t Focal Loss: 0.0003349416965731667 \t Total Loss: 0.06960993847079414\t -Val Focal Loss: 1.1092520172236878e-09 \t Val Total Loss: 0.001837921142578125\n",
            "Epoch 76 \t Focal Loss: 0.0003536117115815897 \t Total Loss: 0.0696041644182351\t -Val Focal Loss: 5.268730214018952e-10 \t Val Total Loss: 0.0018497124314308167\n",
            "Epoch 77 \t Focal Loss: 0.00035134880366427773 \t Total Loss: 0.0695868951382397\t -Val Focal Loss: 9.57222466370565e-10 \t Val Total Loss: 0.0018257087894848415\n",
            "Epoch 78 \t Focal Loss: 0.0003241195004684788 \t Total Loss: 0.06957242107219833\t -Val Focal Loss: 7.345322826170429e-09 \t Val Total Loss: 0.0018569748316492353\n",
            "Epoch 79 \t Focal Loss: 0.0003352414314735357 \t Total Loss: 0.06956890855225728\t -Val Focal Loss: 1.8958326971788405e-10 \t Val Total Loss: 0.0017943446125302995\n",
            "Epoch 80 \t Focal Loss: 0.00035589981494104034 \t Total Loss: 0.06964614833269617\t -Val Focal Loss: 6.230548775322729e-10 \t Val Total Loss: 0.0018780412418501718\n",
            "Epoch 81 \t Focal Loss: 0.0003512549898015723 \t Total Loss: 0.06960311247933683\t -Val Focal Loss: 4.726860173117789e-10 \t Val Total Loss: 0.0018828607031277248\n",
            "Epoch 82 \t Focal Loss: 0.0003373179908921757 \t Total Loss: 0.06959847301887952\t -Val Focal Loss: 2.014466561815555e-10 \t Val Total Loss: 0.0018779090472630091\n",
            "Epoch 83 \t Focal Loss: 0.000306570936640378 \t Total Loss: 0.06957252797998971\t -Val Focal Loss: 1.166096552651652e-09 \t Val Total Loss: 0.0018805650728089469\n",
            "Epoch 84 \t Focal Loss: 0.00029541142994631795 \t Total Loss: 0.06954538575608096\t -Val Focal Loss: 1.7726918599692e-09 \t Val Total Loss: 0.0018414710249219622\n",
            "Epoch 85 \t Focal Loss: 0.00029861901842647017 \t Total Loss: 0.06950541245315572\t -Val Focal Loss: 2.5430223006748876e-09 \t Val Total Loss: 0.0019410552723067148\n",
            "Epoch 86 \t Focal Loss: 0.0002998797257985213 \t Total Loss: 0.06962936817795681\t -Val Focal Loss: 6.464305154655189e-10 \t Val Total Loss: 0.0018097360219274249\n",
            "Epoch 87 \t Focal Loss: 0.00042590386003796775 \t Total Loss: 0.06970200261516536\t -Val Focal Loss: 2.0112042063504173e-09 \t Val Total Loss: 0.0019135160105569022\n",
            "Epoch 88 \t Focal Loss: 0.0003140733178139836 \t Total Loss: 0.06956394718919727\t -Val Focal Loss: 4.404020513350458e-10 \t Val Total Loss: 0.0018714615276881627\n",
            "Epoch 89 \t Focal Loss: 0.000286004038206791 \t Total Loss: 0.06959561211653321\t -Val Focal Loss: 5.060115881860838e-10 \t Val Total Loss: 0.001871715486049652\n",
            "Epoch 90 \t Focal Loss: 0.00029571344445495246 \t Total Loss: 0.06952818220467877\t -Val Focal Loss: 2.3715141342758473e-09 \t Val Total Loss: 0.0018444840397153581\n",
            "Epoch 91 \t Focal Loss: 0.0002788473250543688 \t Total Loss: 0.06950589404742924\t -Val Focal Loss: 4.330324543388997e-10 \t Val Total Loss: 0.001954202353954315\n",
            "Epoch 92 \t Focal Loss: 0.00027287111306570415 \t Total Loss: 0.06962370189080994\t -Val Focal Loss: 1.0962207649559526e-09 \t Val Total Loss: 0.0018388460789408002\n",
            "Epoch 93 \t Focal Loss: 0.0002674933011548452 \t Total Loss: 0.06951022617143693\t -Val Focal Loss: 1.2180801129651236e-10 \t Val Total Loss: 0.0018425617899213518\n",
            "Epoch 94 \t Focal Loss: 0.00030780175844382577 \t Total Loss: 0.06955591618693133\t -Val Focal Loss: 4.818809478430402e-10 \t Val Total Loss: 0.001850914316517966\n",
            "Epoch 95 \t Focal Loss: 0.0002856114380893679 \t Total Loss: 0.06958991318619509\t -Val Focal Loss: 2.4436290451441374e-10 \t Val Total Loss: 0.0018489639673914227\n",
            "Epoch 96 \t Focal Loss: 0.0006827050512436385 \t Total Loss: 0.06994989045339522\t -Val Focal Loss: 9.262902267437312e-09 \t Val Total Loss: 0.001866439197744642\n",
            "Epoch 97 \t Focal Loss: 0.0003686203076417867 \t Total Loss: 0.06965839442053287\t -Val Focal Loss: 1.9051913113798945e-08 \t Val Total Loss: 0.0018385035651070731\n",
            "Epoch 98 \t Focal Loss: 0.000303446679186104 \t Total Loss: 0.06955732188100437\t -Val Focal Loss: 3.3220343555383027e-09 \t Val Total Loss: 0.001849948295525142\n",
            "Epoch 99 \t Focal Loss: 0.0002791705291103351 \t Total Loss: 0.06956292130053043\t -Val Focal Loss: 5.726816912751279e-09 \t Val Total Loss: 0.0018271180135863167\n",
            "Epoch 100 \t Focal Loss: 0.0002706444734374325 \t Total Loss: 0.06956265031916203\t -Val Focal Loss: 6.069753664762954e-09 \t Val Total Loss: 0.0018493209566388812\n"
          ]
        }
      ],
      "source": [
        "model_name = 'MCTrans'\n",
        "# ADNI, Challenge\n",
        "train = 'Challenge'\n",
        "amount = 2\n",
        "dsc_list = []\n",
        "kf = KFold(n_splits=amount)\n",
        "fold_img, fold_seg = get_dataset(train)\n",
        "\n",
        "for k, (train_idx, test_idx) in enumerate(kf.split(fold_img)):\n",
        "    num = 0 if k == 0 else 10 \n",
        "    if k == 0:\n",
        "        # Training data\n",
        "        if train == 'ADNI':\n",
        "            img_temp = np.concatenate([fold_img[train_idx[0]], fold_img[train_idx[1]]])\n",
        "            seg_temp = np.concatenate([fold_seg[train_idx[0]], fold_seg[train_idx[1]]])\n",
        "            img_train = np.concatenate(img_temp[:24])\n",
        "            seg_train = np.concatenate(seg_temp[:24])\n",
        "            img_val = np.concatenate(img_temp[-6:])\n",
        "            seg_val = np.concatenate(seg_temp[-6:])\n",
        "        else:\n",
        "            img_temp = [i[num:num+10] for i in fold_img]\n",
        "            seg_temp = [i[num:num+10] for i in fold_seg]\n",
        "            img_train = np.concatenate([np.concatenate(i[:-2]) for i in img_temp])\n",
        "            seg_train = np.concatenate([np.concatenate(i[:-2]) for i in seg_temp])\n",
        "            img_val = np.concatenate([np.concatenate(i[-2:]) for i in img_temp])\n",
        "            seg_val = np.concatenate([np.concatenate(i[-2:]) for i in seg_temp])\n",
        "\n",
        "        del img_temp, seg_temp\n",
        "        img_train, seg_train = check_data(img_train, seg_train)\n",
        "        img_val, seg_val = check_data(img_val, seg_val)\n",
        "        data_augmentation(img_train, seg_train)\n",
        "        \n",
        "        img_train = img_train.transpose(0,3,1,2); seg_train = seg_train.transpose(0,3,1,2) #Full\n",
        "        img_val = img_val.transpose(0,3,1,2); seg_val = seg_val.transpose(0,3,1,2) #Full\n",
        "\n",
        "        img_train = torch.Tensor(img_train); seg_train = torch.Tensor(seg_train)\n",
        "        img_val = torch.Tensor(img_val); seg_val = torch.Tensor(seg_val)\n",
        "\n",
        "        trainset = torch.utils.data.TensorDataset(img_train,seg_train)\n",
        "        valset = torch.utils.data.TensorDataset(img_val,seg_val)\n",
        "        \n",
        "        trainset = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "        valset = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "        \n",
        "        del img_train, seg_train, img_val, seg_val\n",
        "        \n",
        "        logdir = path+'logdir/{}_{}_fold{}_2_{}_novl_n_points_6'.format(model_name,uniq,k+1,train)\n",
        "        temp = logdir.split('/')[-1]\n",
        "        # Becareful with these 4 lines!\n",
        "        ## Will delete existing folders and files\n",
        "        if len(glob(path+temp)) >= 1: shutil.rmtree(path+temp)\n",
        "        ## Will create folders and files\n",
        "        if len(glob(path+temp)) == 0: os.mkdir(path+temp)\n",
        "\n",
        "        md = pick_model(model_name)\n",
        "        if torch.cuda.is_available():\n",
        "            md = md.cuda()\n",
        "        criterion = TotalLoss(gamma=gamma, alpha=alpha, threshold=threshold)\n",
        "        optimizer = torch.optim.Adam(md.parameters(), lr=lr)\n",
        "        min_valid_loss = np.inf\n",
        "\n",
        "        for i in range(epoch):\n",
        "            train_loss, train_fl = 0.0, 0.0\n",
        "            md.train()     # Optional when not using Model Specific layer\n",
        "            for data, labels in trainset:\n",
        "                if torch.cuda.is_available():\n",
        "                    data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                target, logits = md(data, labels)\n",
        "                \n",
        "                kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "                fl, loss = criterion(target, logits, labels)\n",
        "                loss = loss + kl\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                train_fl += fl.item()\n",
        "            \n",
        "            valid_loss, valid_fl = 0.0, 0.0\n",
        "            md.eval()     # Optional when not using Model Specific layer\n",
        "            for data, labels in valset:\n",
        "                if torch.cuda.is_available():\n",
        "                    data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "                target, logits = md.sampling(data, labels)\n",
        "                kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "                fl, loss = criterion(target, logits, labels)\n",
        "                loss = loss + kl\n",
        "                valid_fl = fl.item() * data.size(0)\n",
        "                valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "            print(f'Epoch {i+1} \\t Focal Loss: {train_fl / len(trainset)} \\t Total Loss: {train_loss / len(trainset)}', end='\\t -')\n",
        "            print(f'Val Focal Loss: {valid_fl / len(valset)} \\t Val Total Loss: {valid_loss / len(valset)}')\n",
        "            if min_valid_loss > valid_fl:\n",
        "                min_valid_loss = valid_fl\n",
        "                # Saving State Dict\n",
        "                torch.save(md.state_dict(), path+temp+\"/checkpoint-best.pth\")\n",
        "\n",
        "#     # Testing\n",
        "    # md.load_state_dict(torch.load(path+temp+\"/checkpoint-best.pth\"))\n",
        "\n",
        "#     ## ADNI Testing\n",
        "#     if train == 'ADNI':\n",
        "#         dsc_patient = []\n",
        "#         paths = glob(path_dataset+'ADNI/{}/*'.format('fold'+str(test_idx[0]+1)))\n",
        "#         paths = paths + glob(path_dataset+'ADNI/{}/*'.format('fold'+str(test_idx[1]+1)))\n",
        "#         paths.sort(key=natural_keys)\n",
        "\n",
        "#         img_test = np.concatenate([fold_img[test_idx[0]], fold_img[test_idx[1]]])\n",
        "#         seg_test = np.concatenate([fold_seg[test_idx[0]], fold_seg[test_idx[1]]])\n",
        "\n",
        "#         for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "#             img = img.transpose(0,3,1,2)\n",
        "#             seg = seg.transpose(0,3,1,2)\n",
        "#             img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "#             testset = torch.utils.data.TensorDataset(img,seg)\n",
        "#             testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "#                                                   shuffle=False, num_workers=2)\n",
        "            \n",
        "#             y_pred_list = []\n",
        "#             y_ss_list, y_sy_list = [], []\n",
        "#             for img_, seg_ in testset:\n",
        "#                 if torch.cuda.is_available():\n",
        "#                     img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "#                 if lr_latent is not None:\n",
        "#                     y_gen = []\n",
        "#                     for x in range(generate_pred):\n",
        "#                         if model_name == 'Prob_MCTrans':\n",
        "#                             y_pred,_ = md.sampling(img_)\n",
        "#                             y_pred = y_pred.cpu().data.numpy()\n",
        "#                         else:\n",
        "#                             y_pred = md.predict(img_)\n",
        "#                         y_gen.append(y_pred)\n",
        "\n",
        "#                     y_gen = np.array(y_gen)\n",
        "#                     y_ss = ambiguity_map(y_gen)\n",
        "#                     y_sy = ambiguity_map(y_gen, seg_.cpu().data.numpy())\n",
        "#                     y_pred = np.average(y_gen, axis=0)\n",
        "\n",
        "#                 else:\n",
        "#                     if model_name == 'MCTrans':\n",
        "#                         y_pred,_ = md(img_, seg_)\n",
        "#                         y_pred = y_pred.cpu().data.numpy()\n",
        "#                     else:\n",
        "#                         y_pred = md.predict(img_)\n",
        "#                     y_ss = None; y_sy = None\n",
        "                    \n",
        "#                 y_pred[y_pred > threshold] = 1\n",
        "#                 y_pred[y_pred <= threshold] = 0\n",
        "#                 y_pred_list.append(y_pred)\n",
        "#                 if lr_latent is not None:\n",
        "#                     y_ss_list.append(y_ss)\n",
        "#                     y_sy_list.append(y_sy)\n",
        "\n",
        "#             y_pred = np.concatenate(y_pred_list)\n",
        "#             y_pred = y_pred.transpose(0,2,3,1)\n",
        "#             if lr_latent is not None:\n",
        "#                 y_ss = np.concatenate(y_ss_list)\n",
        "#                 y_sy = np.concatenate(y_sy_list)\n",
        "#                 y_ss = y_ss.transpose(0,2,3,1)\n",
        "#                 y_sy = y_sy.transpose(0,2,3,1)\n",
        "\n",
        "#             # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "#             seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "#             dsc = dice_coef(seg, y_pred).numpy()\n",
        "#             dsc_patient.append(dsc)\n",
        "            \n",
        "#             patient_id = paths[idx_p].split('/')[-1]\n",
        "#             fold_id = paths[idx_p].split('/')[-2]\n",
        "#             file_in = '{}/{}_wmh.nii.gz'.format(paths[idx_p], patient_id)\n",
        "#             file_out = result_path.format('ADNI', fold_id, patient_id)\n",
        "#             if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "#             file_out_pred = file_out+'/{}_wmh_{}.nii.gz'.format(patient_id, model_name)\n",
        "#             save_wmh(y_pred, file_in, file_out_pred)\n",
        "#             # show_img(img, seg, y_pred, 18)\n",
        "\n",
        "#             # Ambiguity maps\n",
        "#             if y_ss is not None and y_sy is not None:\n",
        "#                 file_out_1 = file_out+'/{}_am_ss_{}.nii.gz'.format(patient_id, model_name)\n",
        "#                 file_out_2 = file_out+'/{}_am_sy_{}.nii.gz'.format(patient_id, model_name)\n",
        "#                 save_wmh(y_ss, file_in, file_out_1)\n",
        "#                 save_wmh(y_sy, file_in, file_out_2)\n",
        "\n",
        "#     ## Challenge Testing\n",
        "#     else:\n",
        "#         dsc_patient = []\n",
        "#         for iter_,inst in enumerate(['Singapore', 'GE3T', 'Utrecht']):\n",
        "#             dsc_temp = []\n",
        "#             paths = glob(path_dataset+'Challenge/{}/*'.format(inst))\n",
        "#             paths.sort(key=natural_keys)\n",
        "#             paths = paths[10-num:20-num]\n",
        "            \n",
        "#             img_test = np.array(fold_img[iter_][10-num:20-num], dtype='float32')\n",
        "#             seg_test = np.array(fold_seg[iter_][10-num:20-num], dtype='float32')\n",
        "\n",
        "#             for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "#                 img = img.transpose(0,3,1,2)\n",
        "#                 seg = seg.transpose(0,3,1,2)\n",
        "#                 img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "#                 testset = torch.utils.data.TensorDataset(img,seg)\n",
        "#                 testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "#                                                     shuffle=False, num_workers=2)\n",
        "                \n",
        "#                 y_pred_list = []\n",
        "#                 y_ss_list, y_sy_list = [], []\n",
        "#                 for img_, seg_ in testset:\n",
        "#                     if torch.cuda.is_available():\n",
        "#                         img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "#                     if lr_latent is not None:\n",
        "#                         y_gen = []\n",
        "#                         for x in range(generate_pred):\n",
        "#                             if model_name == 'Prob_MCTrans':\n",
        "#                                 y_pred,_ = md.sampling(img_)\n",
        "#                                 y_pred = y_pred.cpu().data.numpy()\n",
        "#                             else:\n",
        "#                                 y_pred = md.predict(img_)\n",
        "#                             y_gen.append(y_pred)\n",
        "\n",
        "#                         y_gen = np.array(y_gen)\n",
        "#                         y_ss = ambiguity_map(y_gen)\n",
        "#                         y_sy = ambiguity_map(y_gen, seg_.cpu().data.numpy())\n",
        "#                         y_pred = np.average(y_gen, axis=0)\n",
        "\n",
        "#                     else:\n",
        "#                         if model_name == 'MCTrans':\n",
        "#                             y_pred,_ = md(img_, seg_)\n",
        "#                             y_pred = y_pred.cpu().data.numpy()\n",
        "#                         else:\n",
        "#                             y_pred = md.predict(img_)\n",
        "#                         y_ss = None; y_sy = None\n",
        "                        \n",
        "#                     y_pred[y_pred > threshold] = 1\n",
        "#                     y_pred[y_pred <= threshold] = 0\n",
        "#                     y_pred_list.append(y_pred)\n",
        "#                     if lr_latent is not None:\n",
        "#                         y_ss_list.append(y_ss)\n",
        "#                         y_sy_list.append(y_sy)\n",
        "\n",
        "#                 y_pred = np.concatenate(y_pred_list)\n",
        "#                 y_pred = y_pred.transpose(0,2,3,1)\n",
        "#                 if lr_latent is not None:\n",
        "#                     y_ss = np.concatenate(y_ss_list)\n",
        "#                     y_sy = np.concatenate(y_sy_list)\n",
        "#                     y_ss = y_ss.transpose(0,2,3,1)\n",
        "#                     y_sy = y_sy.transpose(0,2,3,1)\n",
        "\n",
        "#                 # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "#                 seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "#                 dsc = dice_coef(seg, y_pred).numpy()\n",
        "#                 dsc_temp.append(dsc)\n",
        "                \n",
        "#                 patient_id = paths[idx_p].split('/')[-1]\n",
        "#                 file_in = '{}/{}_wmh.nii.gz'.format(paths[idx_p], patient_id)\n",
        "#                 file_out = result_path.format('Challenge', inst, patient_id)\n",
        "#                 if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "#                 file_out_pred = file_out+'/{}_wmh_{}.nii.gz'.format(patient_id, model_name)\n",
        "#                 save_wmh_challenge(y_pred, file_in, file_out_pred, name=inst)\n",
        "#                 # show_img(img, seg, y_pred, 28)\n",
        "\n",
        "#                 # Ambiguity maps\n",
        "#                 if y_ss is not None and y_sy is not None:\n",
        "#                     file_out_1 = file_out+'/{}_am_ss_{}.nii.gz'.format(patient_id, model_name)\n",
        "#                     file_out_2 = file_out+'/{}_am_sy_{}.nii.gz'.format(patient_id, model_name)\n",
        "#                     save_wmh_challenge(y_ss, file_in, file_out_1, name=inst)\n",
        "#                     save_wmh_challenge(y_sy, file_in, file_out_2, name=inst)\n",
        "\n",
        "#             dsc_patient.append(dsc_temp)\n",
        "#     dsc_list.append(dsc_patient)\n",
        "#     print('Average Dice Coefficient on fold-{} : {:.4f}'.format(k+1, np.average(dsc_patient)))\n",
        "# print('Average Dice Coefficient : {:.4f}'.format(np.average(dsc_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s32_KhTK0sM2"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjYfdBCV1oEM",
        "outputId": "228b9c93-a43c-42d3-fca6-e470455d79d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Dice Coefficient on fold-1 : 0.6153\n",
            "Average Dice Coefficient on fold-2 : 0.7170\n",
            "Average Dice Coefficient : 0.6661\n"
          ]
        }
      ],
      "source": [
        "model_name = 'MCTrans'\n",
        "# ADNI, Challenge\n",
        "train = 'Challenge'\n",
        "amount = 2\n",
        "dsc_list = []\n",
        "kf = KFold(n_splits=amount)\n",
        "fold_img, fold_seg = get_dataset(train)\n",
        "\n",
        "for k, (train_idx, test_idx) in enumerate(kf.split(fold_img)):\n",
        "    num = 0 if k == 0 else 10 \n",
        "    # # Training data \n",
        "    # if train == 'ADNI':\n",
        "    #     img_temp = np.concatenate([fold_img[train_idx[0]], fold_img[train_idx[1]]])\n",
        "    #     seg_temp = np.concatenate([fold_seg[train_idx[0]], fold_seg[train_idx[1]]])\n",
        "    #     img_train = np.concatenate(img_temp[:24])\n",
        "    #     seg_train = np.concatenate(seg_temp[:24])\n",
        "    #     img_val = np.concatenate(img_temp[-6:])\n",
        "    #     seg_val = np.concatenate(seg_temp[-6:])\n",
        "    # else:\n",
        "    #     img_temp = [i[num:num+10] for i in fold_img]\n",
        "    #     seg_temp = [i[num:num+10] for i in fold_seg]\n",
        "    #     img_train = np.concatenate([np.concatenate(i[:-2]) for i in img_temp])\n",
        "    #     seg_train = np.concatenate([np.concatenate(i[:-2]) for i in seg_temp])\n",
        "    #     img_val = np.concatenate([np.concatenate(i[-2:]) for i in img_temp])\n",
        "    #     seg_val = np.concatenate([np.concatenate(i[-2:]) for i in seg_temp])\n",
        "\n",
        "    # del img_temp, seg_temp\n",
        "    # img_train, seg_train = check_data(img_train, seg_train)\n",
        "    # img_val, seg_val = check_data(img_val, seg_val)\n",
        "    # data_augmentation(img_train, seg_train)\n",
        "    \n",
        "    # img_train = img_train.transpose(0,3,1,2); seg_train = seg_train.transpose(0,3,1,2) #Full\n",
        "    # img_val = img_val.transpose(0,3,1,2); seg_val = seg_val.transpose(0,3,1,2) #Full\n",
        "\n",
        "    # img_train = torch.Tensor(img_train); seg_train = torch.Tensor(seg_train)\n",
        "    # img_val = torch.Tensor(img_val); seg_val = torch.Tensor(seg_val)\n",
        "\n",
        "    # trainset = torch.utils.data.TensorDataset(img_train,seg_train)\n",
        "    # valset = torch.utils.data.TensorDataset(img_val,seg_val)\n",
        "    \n",
        "    # trainset = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "    #                                        shuffle=True, num_workers=2, drop_last=True)\n",
        "    # valset = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "    #                                      shuffle=False, num_workers=2, drop_last=True)\n",
        "    \n",
        "    # del img_train, seg_train, img_val, seg_val\n",
        "    \n",
        "    logdir = path+'logdir/{}_{}_fold{}_2_{}_novl_n_points_6'.format(model_name,uniq,k+1,train)\n",
        "    temp = logdir.split('/')[-1]\n",
        "    # # Becareful with these 4 lines!\n",
        "    # ## Will delete existing folders and files\n",
        "    # if len(glob(path+temp)) >= 1: shutil.rmtree(path+temp)\n",
        "    # ## Will create folders and files\n",
        "    # if len(glob(path+temp)) == 0: os.mkdir(path+temp)\n",
        "\n",
        "    md = pick_model(model_name)\n",
        "    if torch.cuda.is_available():\n",
        "        md = md.cuda()\n",
        "    # criterion = TotalLoss(gamma=gamma, alpha=alpha, threshold=threshold)\n",
        "    # optimizer = torch.optim.Adam(md.parameters(), lr=lr)\n",
        "    # min_valid_loss = np.inf\n",
        "\n",
        "    # for i in range(epoch):\n",
        "    #     train_loss, train_fl = 0.0, 0.0\n",
        "    #     md.train()     # Optional when not using Model Specific layer\n",
        "    #     for data, labels in trainset:\n",
        "    #         if torch.cuda.is_available():\n",
        "    #             data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    #         optimizer.zero_grad()\n",
        "    #         target, logits = md(data, labels)\n",
        "            \n",
        "    #         kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "    #         fl, loss = criterion(target, logits, labels)\n",
        "    #         loss = loss + kl\n",
        "    #         loss.backward()\n",
        "    #         optimizer.step()\n",
        "    #         train_loss += loss.item()\n",
        "    #         train_fl += fl.item()\n",
        "        \n",
        "    #     valid_loss, valid_fl = 0.0, 0.0\n",
        "    #     md.eval()     # Optional when not using Model Specific layer\n",
        "    #     for data, labels in valset:\n",
        "    #         if torch.cuda.is_available():\n",
        "    #             data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    #         target, logits = md.sampling(data, labels)\n",
        "    #         kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "    #         fl, loss = criterion(target, logits, labels)\n",
        "    #         loss = loss + kl\n",
        "    #         valid_fl = fl.item() * data.size(0)\n",
        "    #         valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "    #     print(f'Epoch {i+1} \\t Focal Loss: {train_fl / len(trainset)} \\t Total Loss: {train_loss / len(trainset)}', end='\\t -')\n",
        "    #     print(f'Val Focal Loss: {valid_fl / len(valset)} \\t Val Total Loss: {valid_loss / len(valset)}')\n",
        "    #     if min_valid_loss > valid_fl:\n",
        "    #         min_valid_loss = valid_fl\n",
        "    #         # Saving State Dict\n",
        "    #         torch.save(md.state_dict(), path+temp+\"/checkpoint-best.pth\")\n",
        "\n",
        "    # Testing\n",
        "    md.load_state_dict(torch.load(path+temp+\"/checkpoint-best.pth\"))\n",
        "\n",
        "    ## ADNI Testing\n",
        "    if train == 'ADNI':\n",
        "        dsc_patient = []\n",
        "        paths = glob(path_dataset+'ADNI/{}/*'.format('fold'+str(test_idx[0]+1)))\n",
        "        paths = paths + glob(path_dataset+'ADNI/{}/*'.format('fold'+str(test_idx[1]+1)))\n",
        "        paths.sort(key=natural_keys)\n",
        "\n",
        "        img_test = np.concatenate([fold_img[test_idx[0]], fold_img[test_idx[1]]])\n",
        "        seg_test = np.concatenate([fold_seg[test_idx[0]], fold_seg[test_idx[1]]])\n",
        "\n",
        "        for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "            img = img.transpose(0,3,1,2)\n",
        "            seg = seg.transpose(0,3,1,2)\n",
        "            img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "            testset = torch.utils.data.TensorDataset(img,seg)\n",
        "            testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "                                                shuffle=False, num_workers=2)\n",
        "            # Probabilistic\n",
        "            if lr_latent is not None:\n",
        "                y_gen = []\n",
        "                for x in range(generate_pred):\n",
        "                    y_pred_patient = []\n",
        "                    for img_, seg_ in testset:\n",
        "                        if torch.cuda.is_available():\n",
        "                            img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                        if model_name == 'Prob_MCTrans':\n",
        "                            y_pred,_ = md.sampling(img_, seg_)\n",
        "                            y_pred = y_pred.cpu().data.numpy()\n",
        "                        else:\n",
        "                            y_pred = md.predict(img_)\n",
        "                        \n",
        "                        y_pred_patient.append(y_pred)\n",
        "                    y_gen.append(np.concatenate(y_pred_patient))\n",
        "                \n",
        "                y_gen = np.array(y_gen)\n",
        "                y_pred = np.average(y_gen, axis=0)\n",
        "                y_gen = y_gen.transpose(0,1,3,4,2)\n",
        "\n",
        "            # Deterministic\n",
        "            else:\n",
        "                y_pred_patient = []\n",
        "                for img_, seg_ in testset:\n",
        "                    if torch.cuda.is_available():\n",
        "                        img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                    if model_name == 'MCTrans':\n",
        "                        y_pred,_ = md.sampling(img_, seg_)\n",
        "                        y_pred = y_pred.cpu().data.numpy()\n",
        "                    else:\n",
        "                        y_pred = md.predict(img_)\n",
        "                    \n",
        "                    y_pred_patient.append(y_pred)\n",
        "                y_pred = np.concatenate(y_pred_patient)\n",
        "\n",
        "            # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "            seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "            y_pred = y_pred.transpose(0,2,3,1)\n",
        "            y_pred[y_pred > threshold] = 1\n",
        "            y_pred[y_pred <= threshold] = 0\n",
        "            dsc = dice_coef(seg, y_pred).numpy()\n",
        "            dsc_patient.append(dsc)\n",
        "            \n",
        "            patient_id = paths[idx_p].split('/')[-1]\n",
        "            fold_id = paths[idx_p].split('/')[-2]\n",
        "            file_in = '{}/{}_wmh.nii.gz'.format(paths[idx_p], patient_id)\n",
        "            file_out = result_path.format('ADNI', fold_id, patient_id)\n",
        "            if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "            file_out_pred = file_out+'/{}_wmh_{}_novl_n_points_6.nii.gz'.format(patient_id, model_name)\n",
        "            save_wmh(y_pred, file_in, file_out_pred)\n",
        "            # show_img(img, seg, y_pred, 18)\n",
        "\n",
        "            # Ambiguity maps\n",
        "            if lr_latent is not None:\n",
        "                file_out_1 = file_out+'/{}_am_ss_{}_test.nii.gz'.format(patient_id, model_name)\n",
        "                file_out_2 = file_out+'/{}_am_sy_{}_test.nii.gz'.format(patient_id, model_name)\n",
        "                y_gen = torch.sigmoid(torch.Tensor(y_gen))\n",
        "                y_gen = torch.clamp(y_gen, 1e-6, 1.0 - 1e-6)\n",
        "                y_gen = y_gen.cpu().data.numpy()\n",
        "                y_ss = ambiguity_map(y_gen)\n",
        "                y_sy = ambiguity_map(y_gen, seg)\n",
        "                save_wmh(y_ss, file_in, file_out_1)\n",
        "                save_wmh(y_sy, file_in, file_out_2)\n",
        "\n",
        "    ## Challenge Testing\n",
        "    else:\n",
        "        dsc_patient = []\n",
        "        for iter_,inst in enumerate(['Singapore', 'GE3T', 'Utrecht']):\n",
        "            dsc_temp = []\n",
        "            paths = glob(path_dataset+'Challenge/{}/*'.format(inst))\n",
        "            paths.sort(key=natural_keys)\n",
        "            paths = paths[10-num:20-num]\n",
        "            \n",
        "            img_test = np.array(fold_img[iter_][10-num:20-num], dtype='float32')\n",
        "            seg_test = np.array(fold_seg[iter_][10-num:20-num], dtype='float32')\n",
        "\n",
        "            for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "                img = img.transpose(0,3,1,2)\n",
        "                seg = seg.transpose(0,3,1,2)\n",
        "                img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "                testset = torch.utils.data.TensorDataset(img,seg)\n",
        "                testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "                                                    shuffle=False, num_workers=2)\n",
        "                \n",
        "                # Probabilistic\n",
        "                if lr_latent is not None:\n",
        "                    y_gen = []\n",
        "                    for x in range(generate_pred):\n",
        "                        y_pred_patient = []\n",
        "                        for img_, seg_ in testset:\n",
        "                            if torch.cuda.is_available():\n",
        "                                img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                            if model_name == 'Prob_MCTrans':\n",
        "                                y_pred,_ = md.sampling(img_, seg_)\n",
        "                                y_pred = y_pred.cpu().data.numpy()\n",
        "                            else:\n",
        "                                y_pred = md.predict(img_)\n",
        "                            \n",
        "                            y_pred_patient.append(y_pred)\n",
        "                        y_gen.append(np.concatenate(y_pred_patient))\n",
        "                    \n",
        "                    y_gen = np.array(y_gen)\n",
        "                    y_pred = np.average(y_gen, axis=0)\n",
        "                    y_gen = y_gen.transpose(0,1,3,4,2)\n",
        "\n",
        "                # Deterministic\n",
        "                else:\n",
        "                    y_pred_patient = []\n",
        "                    for img_, seg_ in testset:\n",
        "                        if torch.cuda.is_available():\n",
        "                            img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                        if model_name == 'MCTrans':\n",
        "                            y_pred,_ = md.sampling(img_, seg_)\n",
        "                            y_pred = y_pred.cpu().data.numpy()\n",
        "                        else:\n",
        "                            y_pred = md.predict(img_)\n",
        "                        \n",
        "                        y_pred_patient.append(y_pred)\n",
        "                    y_pred = np.concatenate(y_pred_patient)\n",
        "\n",
        "                # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "                seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "                y_pred = y_pred.transpose(0,2,3,1)\n",
        "                y_pred[y_pred > threshold] = 1\n",
        "                y_pred[y_pred <= threshold] = 0\n",
        "                dsc = dice_coef(seg, y_pred).numpy()\n",
        "                dsc_temp.append(dsc)\n",
        "                \n",
        "                patient_id = paths[idx_p].split('/')[-1]\n",
        "                file_in = '{}/{}_wmh.nii.gz'.format(paths[idx_p], patient_id)\n",
        "                file_out = result_path.format('Challenge', inst, patient_id)\n",
        "                if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "                file_out_pred = file_out+'/{}_wmh_{}_novl_n_points_6.nii.gz'.format(patient_id, model_name)\n",
        "                save_wmh_challenge(y_pred, file_in, file_out_pred, name=inst)\n",
        "                # show_img(img, seg, y_pred, 28)\n",
        "\n",
        "                # Ambiguity maps\n",
        "                if lr_latent is not None:\n",
        "                    file_out_1 = file_out+'/{}_am_ss_{}.nii.gz'.format(patient_id, model_name)\n",
        "                    file_out_2 = file_out+'/{}_am_sy_{}.nii.gz'.format(patient_id, model_name)\n",
        "                    y_gen = torch.sigmoid(torch.Tensor(y_gen))\n",
        "                    y_gen = torch.clamp(y_gen, 1e-6, 1.0 - 1e-6)\n",
        "                    y_gen = y_gen.cpu().data.numpy()\n",
        "                    y_ss = ambiguity_map(y_gen)\n",
        "                    y_sy = ambiguity_map(y_gen, seg)\n",
        "                    save_wmh_challenge(y_ss, file_in, file_out_1, name=inst)\n",
        "                    save_wmh_challenge(y_sy, file_in, file_out_2, name=inst)\n",
        "\n",
        "            dsc_patient.append(dsc_temp)\n",
        "    dsc_list.append(dsc_patient)\n",
        "    print('Average Dice Coefficient on fold-{} : {:.4f}'.format(k+1, np.average(dsc_patient)))\n",
        "print('Average Dice Coefficient : {:.4f}'.format(np.average(dsc_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBlIPwz1p_LI"
      },
      "source": [
        "# 4.4. Cross Dataset (pytorch model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPWQeqSv97vf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxY01-5sp_Ly",
        "outputId": "1c3be836-4af6-4427-c8c3-e3e928716b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t Focal Loss: 0.005928911717554755 \t Total Loss: 0.0766727959626559\t -Val Focal Loss: 2.744446557237586e-05 \t Val Total Loss: 0.004759978002576686\n",
            "Epoch 2 \t Focal Loss: 0.0020821843313854516 \t Total Loss: 0.07199485993452287\t -Val Focal Loss: 1.2353947038637168e-05 \t Val Total Loss: 0.00412605769598662\n",
            "Epoch 3 \t Focal Loss: 0.0015248975813676405 \t Total Loss: 0.071043419967111\t -Val Focal Loss: 1.2691158787416878e-05 \t Val Total Loss: 0.004429088154835488\n",
            "Epoch 4 \t Focal Loss: 0.0014005334963040545 \t Total Loss: 0.07103456427039725\t -Val Focal Loss: 5.855972169717746e-06 \t Val Total Loss: 0.004113591429012925\n",
            "Epoch 5 \t Focal Loss: 0.0012911004927545172 \t Total Loss: 0.07080224430843303\t -Val Focal Loss: 4.5459609784519495e-06 \t Val Total Loss: 0.004157048107972785\n",
            "Epoch 6 \t Focal Loss: 0.0012288394120615882 \t Total Loss: 0.0707924859912208\t -Val Focal Loss: 5.4278787149032995e-06 \t Val Total Loss: 0.004361250062487019\n",
            "Epoch 7 \t Focal Loss: 0.0011473382629281896 \t Total Loss: 0.07060529351425783\t -Val Focal Loss: 4.948996035243148e-06 \t Val Total Loss: 0.004193281059834494\n",
            "Epoch 8 \t Focal Loss: 0.0011345277985427905 \t Total Loss: 0.07062855182308256\t -Val Focal Loss: 4.927397083574489e-06 \t Val Total Loss: 0.004132205870614123\n",
            "Epoch 9 \t Focal Loss: 0.0010709904806928802 \t Total Loss: 0.07059239051984938\t -Val Focal Loss: 4.027287715545564e-06 \t Val Total Loss: 0.004180890855504505\n",
            "Epoch 10 \t Focal Loss: 0.0010989237510842396 \t Total Loss: 0.07058465736731288\t -Val Focal Loss: 3.3409761720156267e-06 \t Val Total Loss: 0.0040585087306463895\n",
            "Epoch 11 \t Focal Loss: 0.0010030592140397772 \t Total Loss: 0.0705072372482064\t -Val Focal Loss: 3.2209632692358163e-06 \t Val Total Loss: 0.004228341045664318\n",
            "Epoch 12 \t Focal Loss: 0.0009336919635561673 \t Total Loss: 0.0703288544370026\t -Val Focal Loss: 3.4527718037394666e-06 \t Val Total Loss: 0.0041775267515609515\n",
            "Epoch 13 \t Focal Loss: 0.0009314885761592525 \t Total Loss: 0.0703754403761455\t -Val Focal Loss: 1.7934557877536585e-06 \t Val Total Loss: 0.003941849986119057\n",
            "Epoch 14 \t Focal Loss: 0.0008933026179060411 \t Total Loss: 0.07031471242180413\t -Val Focal Loss: 1.8574859523005894e-06 \t Val Total Loss: 0.004456836341032342\n",
            "Epoch 15 \t Focal Loss: 0.0008865608540166353 \t Total Loss: 0.07034142455908307\t -Val Focal Loss: 3.4548952973989855e-06 \t Val Total Loss: 0.003974995506343557\n",
            "Epoch 16 \t Focal Loss: 0.0009073283980669979 \t Total Loss: 0.07036279370323613\t -Val Focal Loss: 3.095952543749738e-06 \t Val Total Loss: 0.004121211037706973\n",
            "Epoch 17 \t Focal Loss: 0.0008203155116728283 \t Total Loss: 0.07021555530556133\t -Val Focal Loss: 1.5258166370843884e-06 \t Val Total Loss: 0.0037593152096022423\n",
            "Epoch 18 \t Focal Loss: 0.0008003543306148697 \t Total Loss: 0.07021417200302235\t -Val Focal Loss: 9.805311791167886e-07 \t Val Total Loss: 0.004203121608762599\n",
            "Epoch 19 \t Focal Loss: 0.0007725678608246313 \t Total Loss: 0.0701805203555101\t -Val Focal Loss: 3.1960371416062117e-06 \t Val Total Loss: 0.004081634888008459\n",
            "Epoch 20 \t Focal Loss: 0.0008385323882208193 \t Total Loss: 0.07025975485244111\t -Val Focal Loss: 2.22154098924305e-06 \t Val Total Loss: 0.004197167371636007\n",
            "Epoch 21 \t Focal Loss: 0.0007484222882610768 \t Total Loss: 0.07020316869140054\t -Val Focal Loss: 2.071540715840103e-06 \t Val Total Loss: 0.004057775682477809\n",
            "Epoch 22 \t Focal Loss: 0.0007788390954599885 \t Total Loss: 0.07023683424746052\t -Val Focal Loss: 1.5273067966423261e-06 \t Val Total Loss: 0.004104046679254788\n",
            "Epoch 23 \t Focal Loss: 0.0007037773486073175 \t Total Loss: 0.07017409056425095\t -Val Focal Loss: 1.4689274448484406e-06 \t Val Total Loss: 0.004085776966009567\n",
            "Epoch 24 \t Focal Loss: 0.0008242846374197218 \t Total Loss: 0.07013157285139993\t -Val Focal Loss: 2.6812696561856723e-06 \t Val Total Loss: 0.003602972671167174\n",
            "Epoch 25 \t Focal Loss: 0.0007031539939420879 \t Total Loss: 0.07011073619653478\t -Val Focal Loss: 1.5790007379489825e-06 \t Val Total Loss: 0.004190128685823127\n",
            "Epoch 26 \t Focal Loss: 0.0006890230993428375 \t Total Loss: 0.07006905083791211\t -Val Focal Loss: 3.4395154433519537e-06 \t Val Total Loss: 0.0040703660516596555\n",
            "Epoch 27 \t Focal Loss: 0.0007740254732400638 \t Total Loss: 0.07023420773243254\t -Val Focal Loss: 1.3715194220265457e-06 \t Val Total Loss: 0.004127306724662211\n",
            "Epoch 28 \t Focal Loss: 0.0006889760544458038 \t Total Loss: 0.07011734139335672\t -Val Focal Loss: 7.152383618835193e-07 \t Val Total Loss: 0.004138960322337364\n",
            "Epoch 29 \t Focal Loss: 0.0006756165308024829 \t Total Loss: 0.07008862637138864\t -Val Focal Loss: 1.0748273726038412e-06 \t Val Total Loss: 0.0042102728317033\n",
            "Epoch 30 \t Focal Loss: 0.0006715950742279952 \t Total Loss: 0.07008914424653612\t -Val Focal Loss: 7.365153795768465e-07 \t Val Total Loss: 0.004187059491427976\n",
            "Epoch 31 \t Focal Loss: 0.0006336990368474996 \t Total Loss: 0.07008239085730733\t -Val Focal Loss: 1.497473524285675e-06 \t Val Total Loss: 0.004094400067827595\n",
            "Epoch 32 \t Focal Loss: 0.0006696929653389141 \t Total Loss: 0.07005705672464248\t -Val Focal Loss: 1.5342228658455632e-06 \t Val Total Loss: 0.003991461956678931\n",
            "Epoch 33 \t Focal Loss: 0.0006430801480204211 \t Total Loss: 0.07004223043401016\t -Val Focal Loss: 1.0180863058333521e-06 \t Val Total Loss: 0.0040511889244193465\n",
            "Epoch 34 \t Focal Loss: 0.0006214751086307397 \t Total Loss: 0.07004638808520801\t -Val Focal Loss: 7.531546258694256e-07 \t Val Total Loss: 0.004078290355739309\n",
            "Epoch 35 \t Focal Loss: 0.0006374878644030351 \t Total Loss: 0.07005863222943837\t -Val Focal Loss: 2.652187545005399e-06 \t Val Total Loss: 0.004148192814926603\n",
            "Epoch 36 \t Focal Loss: 0.0006156871819163707 \t Total Loss: 0.07004073427251599\t -Val Focal Loss: 1.1750452182213984e-06 \t Val Total Loss: 0.004164808276873916\n",
            "Epoch 37 \t Focal Loss: 0.0006970727335663266 \t Total Loss: 0.07008056650551135\t -Val Focal Loss: 1.7793908187035305e-06 \t Val Total Loss: 0.00395639307463347\n",
            "Epoch 38 \t Focal Loss: 0.0005945745271860282 \t Total Loss: 0.07000015690970574\t -Val Focal Loss: 1.757608664937929e-06 \t Val Total Loss: 0.0041156771467692815\n",
            "Epoch 39 \t Focal Loss: 0.000598215086150437 \t Total Loss: 0.06994398846552613\t -Val Focal Loss: 1.4810071634337195e-06 \t Val Total Loss: 0.00419663137464381\n",
            "Epoch 40 \t Focal Loss: 0.0005781125601470481 \t Total Loss: 0.07001292835891151\t -Val Focal Loss: 8.35900562433109e-07 \t Val Total Loss: 0.004121554431630604\n",
            "Epoch 41 \t Focal Loss: 0.0006022046721334825 \t Total Loss: 0.07000140769236735\t -Val Focal Loss: 6.72122340218456e-07 \t Val Total Loss: 0.003953347010398979\n",
            "Epoch 42 \t Focal Loss: 0.000580331249066871 \t Total Loss: 0.06998596476752723\t -Val Focal Loss: 6.346853663658362e-07 \t Val Total Loss: 0.004181492239681643\n",
            "Epoch 43 \t Focal Loss: 0.0006119517082657711 \t Total Loss: 0.0699718597720274\t -Val Focal Loss: 9.555457578624473e-07 \t Val Total Loss: 0.004018384574064568\n",
            "Epoch 44 \t Focal Loss: 0.0005648834645949945 \t Total Loss: 0.06998584079847672\t -Val Focal Loss: 6.80668549716417e-07 \t Val Total Loss: 0.004135349793220634\n",
            "Epoch 45 \t Focal Loss: 0.0005674240542039128 \t Total Loss: 0.06983287825008457\t -Val Focal Loss: 8.436513034307134e-07 \t Val Total Loss: 0.004119197824108067\n",
            "Epoch 46 \t Focal Loss: 0.0005976573570405657 \t Total Loss: 0.0700247429945113\t -Val Focal Loss: 7.95603757150777e-07 \t Val Total Loss: 0.004106800057994786\n",
            "Epoch 47 \t Focal Loss: 0.0005464505932064675 \t Total Loss: 0.06993455515148934\t -Val Focal Loss: 5.215860350724699e-07 \t Val Total Loss: 0.004097149443270555\n",
            "Epoch 48 \t Focal Loss: 0.000608782814359879 \t Total Loss: 0.06999931123293039\t -Val Focal Loss: 1.6381392870515362e-06 \t Val Total Loss: 0.004206419880710431\n",
            "Epoch 49 \t Focal Loss: 0.0005479338444675532 \t Total Loss: 0.06996882385178133\t -Val Focal Loss: 1.265570293736658e-06 \t Val Total Loss: 0.0041526342505839335\n",
            "Epoch 50 \t Focal Loss: 0.0005331191199210455 \t Total Loss: 0.06995513700367359\t -Val Focal Loss: 1.8232363908648935e-06 \t Val Total Loss: 0.004133752032892028\n",
            "Epoch 51 \t Focal Loss: 0.0005278377906023799 \t Total Loss: 0.06992818439083344\t -Val Focal Loss: 1.263281950768806e-06 \t Val Total Loss: 0.0041167509199968024\n",
            "Epoch 52 \t Focal Loss: 0.000566960496327096 \t Total Loss: 0.06995867756023452\t -Val Focal Loss: 1.2829755789099901e-06 \t Val Total Loss: 0.004150158433771845\n",
            "Epoch 53 \t Focal Loss: 0.0005391976827638062 \t Total Loss: 0.06996847110326562\t -Val Focal Loss: 1.044996272174836e-06 \t Val Total Loss: 0.004140691970711324\n",
            "Epoch 54 \t Focal Loss: 0.0005245451486683249 \t Total Loss: 0.06991106491793025\t -Val Focal Loss: 1.2838823587022054e-06 \t Val Total Loss: 0.004132532806538824\n",
            "Epoch 55 \t Focal Loss: 0.0005284734850601981 \t Total Loss: 0.06993886335703572\t -Val Focal Loss: 9.021484280012047e-07 \t Val Total Loss: 0.003974022260352747\n",
            "Epoch 56 \t Focal Loss: 0.0005049616908040074 \t Total Loss: 0.0699450991461786\t -Val Focal Loss: 1.022343609808708e-06 \t Val Total Loss: 0.004139916665518462\n",
            "Epoch 57 \t Focal Loss: 0.0005044362350287284 \t Total Loss: 0.06991149841495349\t -Val Focal Loss: 8.014325050762229e-07 \t Val Total Loss: 0.004130351009653576\n",
            "Epoch 58 \t Focal Loss: 0.0005268380482788425 \t Total Loss: 0.0698958703497631\t -Val Focal Loss: 1.0336923661447173e-06 \t Val Total Loss: 0.00420583851301848\n",
            "Epoch 59 \t Focal Loss: 0.0005585821273099504 \t Total Loss: 0.0699759866402486\t -Val Focal Loss: 1.602584940082491e-06 \t Val Total Loss: 0.004034179804930047\n",
            "Epoch 60 \t Focal Loss: 0.0005230668323125804 \t Total Loss: 0.06993261397626388\t -Val Focal Loss: 1.527167901869363e-06 \t Val Total Loss: 0.004063961221210992\n",
            "Epoch 61 \t Focal Loss: 0.0004906294952646211 \t Total Loss: 0.06990901239658819\t -Val Focal Loss: 9.77737862971236e-07 \t Val Total Loss: 0.004083662335552386\n",
            "Epoch 62 \t Focal Loss: 0.0004923262406860333 \t Total Loss: 0.06986029703845756\t -Val Focal Loss: 1.062063388213682e-06 \t Val Total Loss: 0.0041205135744009445\n",
            "Epoch 63 \t Focal Loss: 0.0004893308309358585 \t Total Loss: 0.06987317896673806\t -Val Focal Loss: 1.4185440446833954e-06 \t Val Total Loss: 0.004097506181517644\n",
            "Epoch 64 \t Focal Loss: 0.0004928900318512689 \t Total Loss: 0.06987939365985497\t -Val Focal Loss: 1.5468018020002811e-06 \t Val Total Loss: 0.004120368566086043\n",
            "Epoch 65 \t Focal Loss: 0.0004937966408829234 \t Total Loss: 0.06989677410638542\t -Val Focal Loss: 9.916897831873885e-07 \t Val Total Loss: 0.004078115100291238\n",
            "Epoch 66 \t Focal Loss: 0.000497349816784834 \t Total Loss: 0.06990303967393229\t -Val Focal Loss: 1.6470614572250242e-06 \t Val Total Loss: 0.0040819013296668205\n",
            "Epoch 67 \t Focal Loss: 0.00047702151539844246 \t Total Loss: 0.06983343182343923\t -Val Focal Loss: 1.6093295355980744e-06 \t Val Total Loss: 0.0037579256207195683\n",
            "Epoch 68 \t Focal Loss: 0.00047986519858395896 \t Total Loss: 0.0699158532673532\t -Val Focal Loss: 1.1926250177810885e-06 \t Val Total Loss: 0.004141857374959917\n",
            "Epoch 69 \t Focal Loss: 0.0004882422287338616 \t Total Loss: 0.06988442743594726\t -Val Focal Loss: 9.230987877765698e-07 \t Val Total Loss: 0.004002162769659242\n",
            "Epoch 70 \t Focal Loss: 0.00046431754820753287 \t Total Loss: 0.06983916967939795\t -Val Focal Loss: 1.956548223814079e-06 \t Val Total Loss: 0.004311793775700811\n",
            "Epoch 71 \t Focal Loss: 0.0004568750183630233 \t Total Loss: 0.069900032162762\t -Val Focal Loss: 1.7775281735542996e-06 \t Val Total Loss: 0.004160494501910993\n",
            "Epoch 72 \t Focal Loss: 0.00044999991972363213 \t Total Loss: 0.06986039644929418\t -Val Focal Loss: 1.7917358382507714e-06 \t Val Total Loss: 0.004045897455357794\n",
            "Epoch 73 \t Focal Loss: 0.0004800748573602145 \t Total Loss: 0.06987055974084148\t -Val Focal Loss: 1.0929104077991153e-06 \t Val Total Loss: 0.004013186515267215\n",
            "Epoch 74 \t Focal Loss: 0.000449087294348063 \t Total Loss: 0.06987684339069632\t -Val Focal Loss: 1.7944117617070564e-06 \t Val Total Loss: 0.004151039603930801\n",
            "Epoch 75 \t Focal Loss: 0.0005633656134495744 \t Total Loss: 0.06991891469058983\t -Val Focal Loss: 1.3019614844277167e-06 \t Val Total Loss: 0.004156067300198683\n",
            "Epoch 76 \t Focal Loss: 0.0004923828103337589 \t Total Loss: 0.06989973114973852\t -Val Focal Loss: 1.1508782879004616e-06 \t Val Total Loss: 0.004041195360582266\n",
            "Epoch 77 \t Focal Loss: 0.0005238533639377713 \t Total Loss: 0.06985660950932801\t -Val Focal Loss: 3.490280994186317e-06 \t Val Total Loss: 0.004196837766846614\n",
            "Epoch 78 \t Focal Loss: 0.0004601062515905012 \t Total Loss: 0.06978575657473139\t -Val Focal Loss: 1.2455525053943048e-06 \t Val Total Loss: 0.004130797154867827\n",
            "Epoch 79 \t Focal Loss: 0.0004340526835087111 \t Total Loss: 0.06982954171744817\t -Val Focal Loss: 1.366168769674181e-06 \t Val Total Loss: 0.004120314743981433\n",
            "Epoch 80 \t Focal Loss: 0.00042759435603164335 \t Total Loss: 0.06983469579111706\t -Val Focal Loss: 1.6826263380431528e-06 \t Val Total Loss: 0.003930907640884172\n",
            "Epoch 81 \t Focal Loss: 0.00042800550261425064 \t Total Loss: 0.06977914346689972\t -Val Focal Loss: 2.5107229472513298e-06 \t Val Total Loss: 0.0039805600892251995\n",
            "Epoch 82 \t Focal Loss: 0.00042442801670868193 \t Total Loss: 0.06977678185409565\t -Val Focal Loss: 2.286506168641595e-06 \t Val Total Loss: 0.004129760300935204\n",
            "Epoch 83 \t Focal Loss: 0.00042866550266968046 \t Total Loss: 0.06979293173498939\t -Val Focal Loss: 1.8082136017228685e-06 \t Val Total Loss: 0.004223051355845893\n",
            "Epoch 84 \t Focal Loss: 0.000426429248502803 \t Total Loss: 0.06980930881075453\t -Val Focal Loss: 1.281391917866891e-06 \t Val Total Loss: 0.003922700881958008\n",
            "Epoch 85 \t Focal Loss: 0.00044105494699582575 \t Total Loss: 0.06983823220979536\t -Val Focal Loss: 1.0661483497303258e-06 \t Val Total Loss: 0.004131482163471962\n",
            "Epoch 86 \t Focal Loss: 0.0004827554218632246 \t Total Loss: 0.06988137909607367\t -Val Focal Loss: 7.739385073907229e-07 \t Val Total Loss: 0.004128551305229984\n",
            "Epoch 87 \t Focal Loss: 0.0004128089794590458 \t Total Loss: 0.06978298134109182\t -Val Focal Loss: 1.1767549596643492e-06 \t Val Total Loss: 0.004287745970398632\n",
            "Epoch 88 \t Focal Loss: 0.0004068173122930965 \t Total Loss: 0.06977588147594688\t -Val Focal Loss: 8.662157312362219e-07 \t Val Total Loss: 0.004065021650115056\n",
            "Epoch 89 \t Focal Loss: 0.0004033158982606603 \t Total Loss: 0.0698263216315265\t -Val Focal Loss: 1.5522164176218212e-06 \t Val Total Loss: 0.00416192234452091\n",
            "Epoch 90 \t Focal Loss: 0.00041471711890112267 \t Total Loss: 0.06984099303260087\t -Val Focal Loss: 2.5554242589052265e-06 \t Val Total Loss: 0.004112894855328461\n",
            "Epoch 91 \t Focal Loss: 0.0004055884241741014 \t Total Loss: 0.06977465203447479\t -Val Focal Loss: 1.4262840346391521e-06 \t Val Total Loss: 0.004245177578570238\n",
            "Epoch 92 \t Focal Loss: 0.00047563265902656816 \t Total Loss: 0.06983802289990514\t -Val Focal Loss: 1.8571975203091974e-06 \t Val Total Loss: 0.004073490847402544\n",
            "Epoch 93 \t Focal Loss: 0.0003982893870990102 \t Total Loss: 0.06976756364418071\t -Val Focal Loss: 1.2927976873036085e-06 \t Val Total Loss: 0.003985584671817609\n",
            "Epoch 94 \t Focal Loss: 0.0003882938973642067 \t Total Loss: 0.06978599839570243\t -Val Focal Loss: 1.3398644453567912e-06 \t Val Total Loss: 0.00413828020665183\n",
            "Epoch 95 \t Focal Loss: 0.0003895302679062639 \t Total Loss: 0.06979045040462985\t -Val Focal Loss: 2.300152607234334e-06 \t Val Total Loss: 0.004137620107451482\n",
            "Epoch 96 \t Focal Loss: 0.0003885847548944543 \t Total Loss: 0.06977684104614809\t -Val Focal Loss: 1.2870673819987186e-06 \t Val Total Loss: 0.0041127422852302664\n",
            "Epoch 97 \t Focal Loss: 0.0003921111948047457 \t Total Loss: 0.06975153491594627\t -Val Focal Loss: 2.709413494622863e-06 \t Val Total Loss: 0.004245174464894764\n",
            "Epoch 98 \t Focal Loss: 0.0004505410314854209 \t Total Loss: 0.06986998370811223\t -Val Focal Loss: 2.5535529260454117e-06 \t Val Total Loss: 0.004136093072037199\n",
            "Epoch 99 \t Focal Loss: 0.00039275722045115834 \t Total Loss: 0.06977599377281976\t -Val Focal Loss: 1.4844284924581203e-06 \t Val Total Loss: 0.0039979628662564866\n",
            "Epoch 100 \t Focal Loss: 0.00037215604818584144 \t Total Loss: 0.06975324363425302\t -Val Focal Loss: 1.3765872893993979e-06 \t Val Total Loss: 0.0043169002034770905\n"
          ]
        }
      ],
      "source": [
        "# ADNI, Challenge\n",
        "train = 'Challenge'\n",
        "model_name = 'MCTrans'\n",
        "# 'ADNI', 'Singapore', 'GE3T', 'Utrecht'\n",
        "test = ['ADNI']\n",
        "\n",
        "fold_img, fold_seg = get_dataset(train)\n",
        "dsc_dict = {i:{j:0 for j in dataset_type} for i in dataset_type}\n",
        "\n",
        "for type_data in [train]: # train data\n",
        "    # Training data\n",
        "    if type_data == 'Challenge':\n",
        "        img_train = np.concatenate([np.concatenate(i[:-2]) for i in fold_img])\n",
        "        seg_train = np.concatenate([np.concatenate(i[:-2]) for i in fold_seg])\n",
        "        img_val = np.concatenate([np.concatenate(i[-2:]) for i in fold_img])\n",
        "        seg_val = np.concatenate([np.concatenate(i[-2:]) for i in fold_seg])\n",
        "    else:\n",
        "        if type_data == 'ADNI':\n",
        "            img_temp = np.concatenate(fold_img)\n",
        "            seg_temp = np.concatenate(fold_seg)\n",
        "        else:\n",
        "            img_temp = fold_img[0]\n",
        "            seg_temp = fold_seg[0]\n",
        "\n",
        "        img_train = np.concatenate(img_temp[:-6])\n",
        "        seg_train = np.concatenate(seg_temp[:-6])\n",
        "        img_val = np.concatenate(img_temp[-6:])\n",
        "        seg_val = np.concatenate(seg_temp[-6:])\n",
        "\n",
        "    img_train, seg_train = check_data(img_train, seg_train)\n",
        "    img_val, seg_val = check_data(img_val, seg_val)\n",
        "    data_augmentation(img_train, seg_train)\n",
        "\n",
        "    img_train = img_train.transpose(0,3,1,2); seg_train = seg_train.transpose(0,3,1,2) #Full\n",
        "    img_val = img_val.transpose(0,3,1,2); seg_val = seg_val.transpose(0,3,1,2) #Full\n",
        "\n",
        "    img_train = torch.Tensor(img_train); seg_train = torch.Tensor(seg_train)\n",
        "    img_val = torch.Tensor(img_val); seg_val = torch.Tensor(seg_val)\n",
        "\n",
        "    trainset = torch.utils.data.TensorDataset(img_train,seg_train)\n",
        "    valset = torch.utils.data.TensorDataset(img_val,seg_val)\n",
        "\n",
        "    trainset = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2, drop_last=True)\n",
        "    valset = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2, drop_last=True)\n",
        "    \n",
        "    del img_train, seg_train, img_val, seg_val\n",
        "\n",
        "    logdir = path+'logdir/{}_{}_novl_n_points_6'.format(model_name, type_data)\n",
        "    temp = logdir.split('/')[-1]\n",
        "    # Becareful with these 4 lines!\n",
        "    ## Will delete existing folders and files\n",
        "    if len(glob(path+temp)) >= 1: shutil.rmtree(path+temp)\n",
        "    ## Will create folders and files\n",
        "    if len(glob(path+temp)) == 0: os.mkdir(path+temp)\n",
        "\n",
        "    md = pick_model(model_name)\n",
        "    if torch.cuda.is_available():\n",
        "        md = md.cuda()\n",
        "    criterion = TotalLoss(gamma=gamma, alpha=alpha, threshold=threshold)\n",
        "    optimizer = torch.optim.Adam(md.parameters(), lr=lr)\n",
        "    min_valid_loss = np.inf\n",
        "\n",
        "    for i in range(epoch):\n",
        "        train_loss, train_fl = 0.0, 0.0\n",
        "        md.train()     # Optional when not using Model Specific layer\n",
        "        for data, labels in trainset:\n",
        "            if torch.cuda.is_available():\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            target, logits = md(data, labels)\n",
        "            \n",
        "            kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "            fl, loss = criterion(target, logits, labels)\n",
        "            loss = loss + kl\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_fl += fl.item()\n",
        "        \n",
        "        valid_loss, valid_fl = 0.0, 0.0\n",
        "        md.eval()     # Optional when not using Model Specific layer\n",
        "        for data, labels in valset:\n",
        "            if torch.cuda.is_available():\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "            target, logits = md.sampling(data, labels)\n",
        "            kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "            fl, loss = criterion(target, logits, labels)\n",
        "            loss = loss + kl\n",
        "            valid_fl = fl.item() * data.size(0)\n",
        "            valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "        print(f'Epoch {i+1} \\t Focal Loss: {train_fl / len(trainset)} \\t Total Loss: {train_loss / len(trainset)}', end='\\t -')\n",
        "        print(f'Val Focal Loss: {valid_fl / len(valset)} \\t Val Total Loss: {valid_loss / len(valset)}')\n",
        "        if min_valid_loss > valid_fl:\n",
        "            min_valid_loss = valid_fl\n",
        "            # Saving State Dict\n",
        "            torch.save(md.state_dict(), path+temp+\"/checkpoint-best.pth\")\n",
        "\n",
        "    # # Testing\n",
        "    # md.load_state_dict(torch.load(path+temp+\"/checkpoint-best.pth\"))\n",
        "    # dsc_ = [i for i in dataset_type if i != type_data and i != 'NITRC']\n",
        "\n",
        "    # for i in test: # test data\n",
        "    #     score = []\n",
        "    #     if i == 'ADNI': paths = glob(path_dataset+'ADNI/*/*')\n",
        "    #     elif i == 'NITRC' : paths = glob(path_dataset+'NITRC/training/*')\n",
        "    #     else: paths = glob(path_dataset+'Challenge/{}/*'.format(i))\n",
        "    #     paths.sort(key=natural_keys)\n",
        "        \n",
        "    #     img_test, seg_test = get_dataset(i)\n",
        "    #     img_test = np.concatenate(img_test)\n",
        "    #     seg_test = np.concatenate(seg_test)\n",
        "\n",
        "    #     for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "    #         img = img.transpose(0,3,1,2)\n",
        "    #         seg = seg.transpose(0,3,1,2)\n",
        "    #         img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "    #         testset = torch.utils.data.TensorDataset(img,seg)\n",
        "    #         testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "    #                                               shuffle=False, num_workers=2)\n",
        "    #         y_pred_list = []\n",
        "    #         y_ss_list, y_sy_list = [], []\n",
        "    #         for img_, seg_ in testset:\n",
        "    #             if torch.cuda.is_available():\n",
        "    #                 img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "    #             if lr_latent is not None:\n",
        "    #                 y_gen = []\n",
        "    #                 for x in range(generate_pred):\n",
        "    #                     if model_name == 'Prob_MCTrans':\n",
        "    #                         y_pred,_ = md.sampling(img_)\n",
        "    #                         y_pred = y_pred.cpu().data.numpy()\n",
        "    #                     else:\n",
        "    #                         y_pred = md.predict(img_)\n",
        "    #                     y_gen.append(y_pred)\n",
        "\n",
        "    #                 y_gen = np.array(y_gen)\n",
        "    #                 y_ss = ambiguity_map(y_gen)\n",
        "    #                 y_sy = ambiguity_map(y_gen, seg_.cpu().data.numpy())\n",
        "    #                 y_pred = np.average(y_gen, axis=0)\n",
        "\n",
        "    #             else:\n",
        "    #                 if model_name == 'MCTrans':\n",
        "    #                     y_pred,_ = md(img_, seg_)\n",
        "    #                     y_pred = y_pred.cpu().data.numpy()\n",
        "    #                 else:\n",
        "    #                     y_pred = md.predict(img_)\n",
        "    #                 y_ss = None; y_sy = None\n",
        "                    \n",
        "    #             y_pred[y_pred > threshold] = 1\n",
        "    #             y_pred[y_pred <= threshold] = 0\n",
        "    #             y_pred_list.append(y_pred)\n",
        "    #             if lr_latent is not None:\n",
        "    #                 y_ss_list.append(y_ss)\n",
        "    #                 y_sy_list.append(y_sy)\n",
        "            \n",
        "    #         y_pred = np.concatenate(y_pred_list)\n",
        "    #         y_pred = y_pred.transpose(0,2,3,1)\n",
        "    #         if lr_latent is not None:\n",
        "    #             y_ss = np.concatenate(y_ss_list)\n",
        "    #             y_sy = np.concatenate(y_sy_list)\n",
        "    #             y_ss = y_ss.transpose(0,2,3,1)\n",
        "    #             y_sy = y_sy.transpose(0,2,3,1)\n",
        "\n",
        "    #         # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "    #         seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "    #         dsc = dice_coef(seg, y_pred).numpy()\n",
        "    #         score.append(dsc)\n",
        "\n",
        "    #         patient_id = paths[idx_p].split('/')[-1]\n",
        "    #         if i == 'ADNI':\n",
        "    #             fold_id = paths[idx_p].split('/')[-2]\n",
        "    #             file_out = result_path.format(i, fold_id, patient_id)\n",
        "    #         elif i == 'NITRC':\n",
        "    #             fold_id = 'training'\n",
        "    #             file_out = result_path.format(i, fold_id, patient_id)\n",
        "    #         else:\n",
        "    #             fold_id = 'Challenge'\n",
        "    #             file_out = result_path.format(fold_id, i, patient_id)\n",
        "\n",
        "    #         temp = 'lesion' if i == 'NITRC' else 'wmh'\n",
        "    #         file_in = '{}/{}_{}.nii.gz'.format(paths[idx_p], patient_id, temp)\n",
        "    #         if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "    #         file_out_pred = file_out+'/{}_wmh_{}_{}.nii.gz'.format(patient_id, model_name,type_data)\n",
        "    #         save_wmh(y_pred, file_in, file_out_pred, name=i)\n",
        "    #         # show_img(img, seg, y_pred, 18)\n",
        "\n",
        "    #         # Ambiguity maps\n",
        "    #         if y_ss is not None and y_sy is not None:\n",
        "    #             file_out_1 = file_out+'/{}_am_ss_{}_{}.nii.gz'.format(patient_id, model_name, type_data)\n",
        "    #             file_out_2 = file_out+'/{}_am_sy_{}_{}.nii.gz'.format(patient_id, model_name, type_data)\n",
        "    #             save_wmh(y_ss, file_in, file_out_1, name=i)\n",
        "    #             save_wmh(y_sy, file_in, file_out_2, name=i)\n",
        "\n",
        "    #     dsc_dict[type_data][i] = np.average(score)\n",
        "    #     print('Dice Coefficient with dataset {} trained by {}: {:.4f}'.format(i, type_data, dsc_dict[type_data][i]))\n",
        "    # avg_ = np.average([dsc_dict[train][i] for i in test])\n",
        "    # print('Average: {:.4f}'.format(avg_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yWxcaMn99lZ"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWxP4nj6fTpq",
        "outputId": "1a680cc4-3db8-439f-a2cd-dcb38c0352af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Coefficient with dataset ADNI trained by Challenge: 0.4071\n",
            "Average: 0.4071\n"
          ]
        }
      ],
      "source": [
        "# ADNI, Challenge\n",
        "train = 'Challenge'\n",
        "model_name = 'MCTrans'\n",
        "# 'ADNI', 'Singapore', 'GE3T', 'Utrecht'\n",
        "test = ['ADNI']\n",
        "\n",
        "# fold_img, fold_seg = get_dataset(train)\n",
        "dsc_dict = {i:{j:0 for j in dataset_type} for i in dataset_type}\n",
        "\n",
        "for type_data in [train]: # train data\n",
        "    # # Training data\n",
        "    # if type_data == 'Challenge':\n",
        "    #     img_train = np.concatenate([np.concatenate(i[:-2]) for i in fold_img])\n",
        "    #     seg_train = np.concatenate([np.concatenate(i[:-2]) for i in fold_seg])\n",
        "    #     img_val = np.concatenate([np.concatenate(i[-2:]) for i in fold_img])\n",
        "    #     seg_val = np.concatenate([np.concatenate(i[-2:]) for i in fold_seg])\n",
        "    # else:\n",
        "    #     if type_data == 'ADNI':\n",
        "    #         img_temp = np.concatenate(fold_img)\n",
        "    #         seg_temp = np.concatenate(fold_seg)\n",
        "    #     else:\n",
        "    #         img_temp = fold_img[0]\n",
        "    #         seg_temp = fold_seg[0]\n",
        "\n",
        "    #     img_train = np.concatenate(img_temp[:-6])\n",
        "    #     seg_train = np.concatenate(seg_temp[:-6])\n",
        "    #     img_val = np.concatenate(img_temp[-6:])\n",
        "    #     seg_val = np.concatenate(seg_temp[-6:])\n",
        "\n",
        "    # img_train, seg_train = check_data(img_train, seg_train)\n",
        "    # img_val, seg_val = check_data(img_val, seg_val)\n",
        "    # data_augmentation(img_train, seg_train)\n",
        "\n",
        "    # img_train = img_train.transpose(0,3,1,2); seg_train = seg_train.transpose(0,3,1,2) #Full\n",
        "    # img_val = img_val.transpose(0,3,1,2); seg_val = seg_val.transpose(0,3,1,2) #Full\n",
        "\n",
        "    # img_train = torch.Tensor(img_train); seg_train = torch.Tensor(seg_train)\n",
        "    # img_val = torch.Tensor(img_val); seg_val = torch.Tensor(seg_val)\n",
        "\n",
        "    # trainset = torch.utils.data.TensorDataset(img_train,seg_train)\n",
        "    # valset = torch.utils.data.TensorDataset(img_val,seg_val)\n",
        "\n",
        "    # trainset = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "    #                                        shuffle=True, num_workers=2, drop_last=True)\n",
        "    # valset = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "    #                                      shuffle=False, num_workers=2, drop_last=True)\n",
        "    \n",
        "    # del img_train, seg_train, img_val, seg_val\n",
        "\n",
        "    logdir = path+'logdir/{}_{}_novl_n_points_6'.format(model_name, type_data)\n",
        "    temp = logdir.split('/')[-1]\n",
        "    # # Becareful with these 4 lines!\n",
        "    # ## Will delete existing folders and files\n",
        "    # if len(glob(path+temp)) >= 1: shutil.rmtree(path+temp)\n",
        "    # ## Will create folders and files\n",
        "    # if len(glob(path+temp)) == 0: os.mkdir(path+temp)\n",
        "\n",
        "    md = pick_model(model_name)\n",
        "    if torch.cuda.is_available():\n",
        "        md = md.cuda()\n",
        "    # criterion = TotalLoss(gamma=gamma, alpha=alpha, threshold=threshold)\n",
        "    # optimizer = torch.optim.Adam(md.parameters(), lr=lr)\n",
        "    # min_valid_loss = np.inf\n",
        "\n",
        "    # for i in range(epoch):\n",
        "    #     train_loss = 0.0\n",
        "    #     md.train()     # Optional when not using Model Specific layer\n",
        "    #     for data, labels in trainset:\n",
        "    #         if torch.cuda.is_available():\n",
        "    #             data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    #         optimizer.zero_grad()\n",
        "    #         target, logits = md(data, labels)\n",
        "            \n",
        "    #         kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "    #         loss = criterion(target, logits, labels) + kl\n",
        "    #         loss.backward()\n",
        "    #         optimizer.step()\n",
        "    #         train_loss += loss.item()\n",
        "        \n",
        "    #     valid_loss = 0.0\n",
        "    #     del data, labels\n",
        "    #     md.eval()     # Optional when not using Model Specific layer\n",
        "    #     for data, labels in valset:\n",
        "    #         if torch.cuda.is_available():\n",
        "    #             data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    #         target, logits = md.sampling(data)\n",
        "    #         kl = beta * torch.mean(md.kl_divergence()) if lr_latent is not None else 0\n",
        "    #         loss = criterion(target, logits, labels) + kl\n",
        "    #         valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "    #     print(f'Epoch {i+1} \\t\\t Training Loss: {train_loss / len(trainset)} \\t\\t Validation Loss: {valid_loss / len(valset)}')\n",
        "    #     if min_valid_loss > valid_loss:\n",
        "    #         min_valid_loss = valid_loss\n",
        "    #         # Saving State Dict\n",
        "    #         torch.save(md.state_dict(), path+temp+\"/checkpoint-best.pth\")\n",
        "\n",
        "    # Testing\n",
        "    md.load_state_dict(torch.load(path+temp+\"/checkpoint-best.pth\"))\n",
        "    dsc_ = [i for i in dataset_type if i != type_data and i != 'NITRC']\n",
        "\n",
        "    for i in test: # test data\n",
        "        score = []\n",
        "        if i == 'ADNI': paths = glob(path_dataset+'ADNI/*/*')\n",
        "        elif i == 'NITRC' : paths = glob(path_dataset+'NITRC/training/*')\n",
        "        else: paths = glob(path_dataset+'Challenge/{}/*'.format(i))\n",
        "        paths.sort(key=natural_keys)\n",
        "        \n",
        "        img_test, seg_test = get_dataset(i)\n",
        "        img_test = np.concatenate(img_test)\n",
        "        seg_test = np.concatenate(seg_test)\n",
        "\n",
        "        for idx_p, (img, seg) in enumerate(zip(img_test, seg_test)):\n",
        "            img = img.transpose(0,3,1,2)\n",
        "            seg = seg.transpose(0,3,1,2)\n",
        "            img = torch.Tensor(img); seg = torch.Tensor(seg)\n",
        "\n",
        "            testset = torch.utils.data.TensorDataset(img,seg)\n",
        "            testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "                                                  shuffle=False, num_workers=2)\n",
        "            # Probabilistic\n",
        "            if lr_latent is not None:\n",
        "                y_gen = []\n",
        "                for x in range(generate_pred):\n",
        "                    y_pred_patient = []\n",
        "                    for img_, seg_ in testset:\n",
        "                        if torch.cuda.is_available():\n",
        "                            img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                        if model_name == 'Prob_MCTrans':\n",
        "                            y_pred,_ = md.sampling(img_, seg_)\n",
        "                            y_pred = y_pred.cpu().data.numpy()\n",
        "                        else:\n",
        "                            y_pred = md.predict(img_)\n",
        "                        \n",
        "                        y_pred_patient.append(y_pred)\n",
        "                    y_gen.append(np.concatenate(y_pred_patient))\n",
        "                \n",
        "                y_gen = np.array(y_gen)\n",
        "                y_pred = np.average(y_gen, axis=0)\n",
        "                y_gen = y_gen.transpose(0,1,3,4,2)\n",
        "\n",
        "            # Deterministic\n",
        "            else:\n",
        "                y_pred_patient = []\n",
        "                for img_, seg_ in testset:\n",
        "                    if torch.cuda.is_available():\n",
        "                        img_, seg_ = img_.cuda(), seg_.cuda()\n",
        "\n",
        "                    if model_name == 'MCTrans':\n",
        "                        y_pred,_ = md.sampling(img_, seg_)\n",
        "                        y_pred = y_pred.cpu().data.numpy()\n",
        "                    else:\n",
        "                        y_pred = md.predict(img_)\n",
        "                    \n",
        "                    y_pred_patient.append(y_pred)\n",
        "                y_pred = np.concatenate(y_pred_patient)\n",
        "\n",
        "            # img = img.cpu().data.numpy().transpose(0,2,3,1)\n",
        "            seg = seg.cpu().data.numpy().transpose(0,2,3,1)\n",
        "            y_pred = y_pred.transpose(0,2,3,1)\n",
        "            y_pred[y_pred > threshold] = 1\n",
        "            y_pred[y_pred <= threshold] = 0\n",
        "            dsc = dice_coef(seg, y_pred).numpy()\n",
        "            score.append(dsc)\n",
        "\n",
        "            patient_id = paths[idx_p].split('/')[-1]\n",
        "            if i == 'ADNI':\n",
        "                fold_id = paths[idx_p].split('/')[-2]\n",
        "                file_out = result_path.format(i, fold_id, patient_id)\n",
        "            elif i == 'NITRC':\n",
        "                fold_id = 'training'\n",
        "                file_out = result_path.format(i, fold_id, patient_id)\n",
        "            else:\n",
        "                fold_id = 'Challenge'\n",
        "                file_out = result_path.format(fold_id, i, patient_id)\n",
        "\n",
        "            temp = 'lesion' if i == 'NITRC' else 'wmh'\n",
        "            file_in = '{}/{}_{}.nii.gz'.format(paths[idx_p], patient_id, temp)\n",
        "            if len(glob(file_out)) == 0: os.makedirs(file_out)\n",
        "            file_out_pred = file_out+'/{}_wmh_{}_{}_novl_n_points_6.nii.gz'.format(patient_id, model_name,type_data)\n",
        "            save_wmh(y_pred, file_in, file_out_pred, name=i)\n",
        "            # show_img(img, seg, y_pred, 18)\n",
        "\n",
        "            # Ambiguity maps\n",
        "            if lr_latent is not None:\n",
        "                file_out_1 = file_out+'/{}_am_ss_{}_{}.nii.gz'.format(patient_id, model_name, type_data)\n",
        "                file_out_2 = file_out+'/{}_am_sy_{}_{}.nii.gz'.format(patient_id, model_name, type_data)\n",
        "                y_gen = torch.sigmoid(torch.Tensor(y_gen))\n",
        "                y_gen = torch.clamp(y_gen, 1e-6, 1.0 - 1e-6)\n",
        "                y_gen = y_gen.cpu().data.numpy()\n",
        "                y_ss = ambiguity_map(y_gen)\n",
        "                y_sy = ambiguity_map(y_gen, seg)\n",
        "                save_wmh(y_ss, file_in, file_out_1, name=i)\n",
        "                save_wmh(y_sy, file_in, file_out_2, name=i)\n",
        "\n",
        "        dsc_dict[type_data][i] = np.average(score)\n",
        "        print('Dice Coefficient with dataset {} trained by {}: {:.4f}'.format(i, type_data, dsc_dict[type_data][i]))\n",
        "    avg_ = np.average([dsc_dict[train][i] for i in test])\n",
        "    print('Average: {:.4f}'.format(avg_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSrX-l_R21xH"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXdbxyrv26EG"
      },
      "outputs": [],
      "source": [
        "# 'ADNI', 'Challenge'\n",
        "train = 'Challenge'\n",
        "# 'ADNI', 'Singapore', 'GE3T', 'Utrecht', 'Challenge'\n",
        "test = ['ADNI']\n",
        "# U_Net, AttU_Net, XNet, AttXNet, KiU_Net, Prob_U_Net, H_Prob_U_Net, PHiSeg\n",
        "model_name = 'MCTrans'\n",
        "df_by_model, dsc, patient = [], [], []\n",
        "\n",
        "for i in test: # testing \n",
        "    if i == 'ADNI': \n",
        "        paths = glob(path_dataset+'ADNI/*/*/*wmh.nii.gz')\n",
        "    elif i == 'NITRC' : \n",
        "        paths = glob(path_dataset+'NITRC/training/*/*lesion.nii.gz')\n",
        "    elif i == 'Challenge':\n",
        "        paths = []\n",
        "        for j in ['Singapore', 'GE3T', 'Utrecht']:\n",
        "            path = glob(path_dataset+'Challenge/{}/*/*wmh.nii.gz'.format(j))\n",
        "            path.sort(key=natural_keys)\n",
        "            paths += path\n",
        "    else: \n",
        "        paths = glob(path_dataset+'Challenge/{}/*/*wmh.nii.gz'.format(i))\n",
        "\n",
        "    if i != 'Challenge': paths.sort(key=natural_keys)\n",
        "    temp, temp_dsc, temp_patient = [], [], []\n",
        "\n",
        "    for lp in paths:\n",
        "        patient_id = lp.split('/')[-2]\n",
        "        if i == 'ADNI':\n",
        "            fold_id = lp.split('/')[-3]\n",
        "            pp = result_path.format(i, fold_id, patient_id)\n",
        "        elif i == 'NITRC':\n",
        "            pp = result_path.format(i, 'training', patient_id)\n",
        "        elif i == 'Challenge':\n",
        "            fold_id = lp.split('/')[-3]\n",
        "            pp = result_path.format('Challenge', fold_id, patient_id)\n",
        "        else:\n",
        "            pp = result_path.format('Challenge', i, patient_id)\n",
        "        \n",
        "        if i == train:\n",
        "            pp = pp+'/{}_wmh_{}_novl_n_points_6.nii.gz'.format(patient_id, model_name) # training\n",
        "        else:\n",
        "            pp = pp+'/{}_wmh_{}_{}_novl_n_points_6.nii.gz'.format(patient_id, model_name, train) # training\n",
        "\n",
        "        voxel_true = nib.load(lp); wmh_true = read_data(lp)\n",
        "        voxel_pred = nib.load(pp); wmh_pred = read_data(pp)\n",
        "        volume_true = np.prod([abs(voxel_true.affine[i][i]) for i in range(3)])/1000\n",
        "        volume_true = np.count_nonzero(wmh_true)*volume_true\n",
        "        volume_pred = np.prod([abs(voxel_pred.affine[i][i]) for i in range(3)])/1000\n",
        "        volume_pred = np.count_nonzero(wmh_pred)*volume_pred\n",
        "        score = dice_coef(wmh_true, wmh_pred).numpy()\n",
        "        temp_dsc.append(score)\n",
        "        temp_patient.append(patient_id)\n",
        "        temp.append([volume_true, volume_pred])\n",
        "    df_by_model.append(temp)\n",
        "    dsc.append(temp_dsc)\n",
        "    patient.append(temp_patient)\n",
        "\n",
        "df_temp = [pd.DataFrame(i, index=j).transpose() for i,j in zip(df_by_model, patient)]\n",
        "temp_str = '_Cross' if train not in test else ''\n",
        "with pd.ExcelWriter(eval_path+'output{}_{}_{}_novl_n_points_6.xlsx'.format(temp_str, train, model_name)) as writter:\n",
        "    num = 0\n",
        "    for k,i in enumerate(test):\n",
        "        df_temp[k].to_excel(writter, startrow=num, startcol=0, sheet_name='vol')\n",
        "        pd.DataFrame([dsc[k]], columns=df_temp[k].columns).to_excel(\n",
        "            writter, startrow=num, startcol=0, sheet_name='dsc')\n",
        "        num = num + df_temp[k].shape[0] + 3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "n2fs4Ps8GY0Y",
        "Q2dned6EHXdd",
        "10MH0TkmrTXG",
        "m-e4SiGnAgLv",
        "GSse6KJA_uxV",
        "utSk7LxJDB4y",
        "Z2KdAG1iCFyD",
        "jKr7vaEgHlM5",
        "9wGprImIEOCK",
        "jJlqb9HAF8wm",
        "KUn-TZURGKyg",
        "pYhyXm1BGqlR",
        "JIv-NOL1XLih",
        "LrLlkkca0pCc",
        "s32_KhTK0sM2",
        "KPWQeqSv97vf",
        "8yWxcaMn99lZ",
        "wSrX-l_R21xH"
      ],
      "machine_shape": "hm",
      "name": "Transformer-based Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
